{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work3/s174498/final/linear_head/checkpoint-1500\n"
     ]
    }
   ],
   "source": [
    "#import warnings\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "\n",
    "sys.path.insert(0,'/zhome/94/5/127021/speciale/master_project')\n",
    "from src.models.hate_tcav.TCAV import get_preds_tcavs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "datadir = '/work3/s174498/sst2_dataset/'\n",
    "N = 200\n",
    "#test_0_dataset = load_from_disk(datadir + 'test_0_dataset')\n",
    "test_1_dataset = load_from_disk(datadir + 'test_1_dataset')\n",
    "\n",
    "filename = \"/work3/s174498/sst2_dataset/positive\"\n",
    "\n",
    "ds_pos = load_from_disk(filename)\n",
    "const_len = N\n",
    "ds_pos_text = ds_pos['sentence']\n",
    "\n",
    "filename = \"/work3/s174498/sst2_dataset/negative\"\n",
    "ds_neg = load_from_disk(filename)\n",
    "ds_neg_text = ds_neg['sentence']\n",
    "\n",
    "pos = [ds_pos_text[i] for i in list(np.random.choice(len(ds_pos_text),30))]\n",
    "neg = [ds_neg_text[i] for i in list(np.random.choice(len(ds_neg_text),30))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test model load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_Model ='/work3/s174498/final/linear_head/checkpoint-1500' #'/zhome/94/5/127021/speciale/master_project/src/models/hate_tcav/models/'\n",
    "\n",
    "folder = PATH_TO_Model#+'exp-Toxic-roberta'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> load model \n",
      ">>> model_folder_toxic /work3/s174498/final/linear_head/checkpoint-1500\n",
      ">>> load RSM /work3/s174498/final/linear_head/checkpoint-1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /work3/s174498/final/linear_head/checkpoint-1500 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> load tokenizer \n",
      ">>> model loaded \n",
      ">>> random data \n",
      "calculating cavs...\n",
      "calculating logits and grads...\n",
      "/zhome/94/5/127021/speciale/master_project/notebooks\n",
      "TCAV score for the concept: \n",
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "logits,sensitivity,TCAV = get_preds_tcavs(classifier = 'toxicity',desired_class = 1,examples_set = 'random',concept_examples = neg,num_runs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'yield' outside function (2979096002.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [2], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    yield text_indices\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'yield' outside function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for input_text in ds_text:\n",
    "    text_idx = tokenizer_checkpoint.encode(input_text)\n",
    "    text_idx += [1] * max(0, const_len - len(text_idx))\n",
    "    input_tensor = torch.tensor(text_idx, device=device).unsqueeze(0)\n",
    "    input_tensors.append(input_tensor)\n",
    "    #text_idx = tokenizer_checkpoint.tokenize(ds_text[i])\n",
    "    #text_idx[i] = tokenizer_checkpoint.encode(text_idx[i])\n",
    "    text_idx = tokenizer_checkpoint.encode(ds_text[i])\n",
    "    text_idx += [1] * max(0, const_len - len(text_idx[i]))\n",
    "    text_indices = torch.tensor(text_idx, device=device).unsqueeze(0)\n",
    "    yield text_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('roberta_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c3ec90920587dcd62ca10f98568309ae5fe8dd1757bd16b3e1a83d20ad0c067"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
