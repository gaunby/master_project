{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer\n",
    "from datasets import load_from_disk, load_metric, Dataset, load_dataset\n",
    "\n",
    "from captum.concept import TCAV\n",
    "from captum.concept._utils.data_iterator import dataset_to_dataloader, CustomIterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /work3/s174498/final/linear_head/checkpoint-1500 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = '/work3/s174498/final/linear_head/checkpoint-1500' #finetuning-sentiment-model-test-head3/checkpoint-500'\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(checkpoint,output_hidden_states = True,return_dict = True,)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer_checkpoint = RobertaTokenizer.from_pretrained(checkpoint) \n",
    "#tokenizer_pretrained = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "datadir = '/work3/s174498/sst2_dataset/'\n",
    "test_0_dataset = load_from_disk(datadir + 'test_0_dataset')\n",
    "test_1_dataset = load_from_disk(datadir + 'test_1_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='/work3/s174498/final/linear_head/checkpoint-1500', vocab_size=50265, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 102,\n",
       " 1099,\n",
       " 1203,\n",
       " 77,\n",
       " 51,\n",
       " 128,\n",
       " 241,\n",
       " 3518,\n",
       " 7,\n",
       " 28,\n",
       " 519,\n",
       " 10,\n",
       " 6981,\n",
       " 1144,\n",
       " 908,\n",
       " 1437,\n",
       " 2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_checkpoint.encode(test_0_dataset['sentence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaAttention(\n",
       "  (self): RobertaSelfAttention(\n",
       "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (output): RobertaSelfOutput(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roberta.encoder.layer[0].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_pretrained.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/94/5/127021/miniconda3/envs/roberta_env/lib/python3.9/site-packages/captum/concept/_utils/classifier.py:130: UserWarning: Using default classifier for TCAV which keeps input both train and test datasets in the memory. Consider defining your own classifier that doesn't rely heavily on memory, for large number of concepts, by extending `Classifer` abstract class\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tcav = TCAV(model, layers=['roberta.encoder.layer[0]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the text inputs for the model\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer_checkpoint(examples[\"sentence\"], truncation=True)\n",
    "\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 0\n",
    "for i in range(len(test_0_dataset['sentence'])):\n",
    "    j = len(test_0_dataset['sentence'][i])\n",
    "    if j > N :\n",
    "        N = j\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_0_dataset['sentence'][0]= test_0_dataset['sentence'][0][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he', 'j', 'Ġmed', 'Ġdig'],\n",
       " ['he', 'j', 'Ġmed', 'Ġdig'],\n",
       " ['he', 'j', 'Ġmed', 'Ġdig'],\n",
       " ['he', 'j', 'Ġmed', 'Ġdig']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 'hej med dig'\n",
    "L = []\n",
    "for i in range(4):\n",
    "    L.append(tokenizer_checkpoint.tokenize(t))\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hej', 'hej', 'hej', 'hej', 'd', 'd', 'd']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L+= ['d']*3\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_from_filename(filename):\n",
    "    datadir = '/work3/s174498/sst2_dataset/'\n",
    "    ds = load_from_disk(datadir + filename)\n",
    "    const_len = N\n",
    "    ds_text = ds['sentence']\n",
    "    text_idx = []\n",
    "    for i in range(len(ds_text)):\n",
    "        text_idx.append(tokenizer_checkpoint.encode(ds_text[i]))\n",
    "        \n",
    "    concept.text = concept.text[:const_len]\n",
    "        concept.text += ['<pad>'] * max(0, const_len - len(concept.text))\n",
    "        text_indices = torch.tensor([TEXT.vocab.stoi[t] for t in concept.text], device=device)\n",
    "        yield text_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hvordan bruger man den her : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_from_filename(filename):\n",
    "    ds = torchtext.data.TabularDataset(path=filename,\n",
    "                                       fields=[('text', torchtext.data.Field()),\n",
    "                                               ('label', torchtext.data.Field())],\n",
    "                                       format='csv')\n",
    "    const_len = 7\n",
    "    for concept in ds:\n",
    "        concept.text = concept.text[:const_len]\n",
    "        concept.text += ['pad'] * max(0, const_len - len(concept.text))\n",
    "        text_indices = torch.tensor([TEXT.vocab.stoi[t] for t in concept.text], device=device)\n",
    "        yield text_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_from_filename(filename):\n",
    "    img = Image.open(filename).convert(\"RGB\")\n",
    "    return transform(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hent concept data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_concept(name, id, concepts_path=\"/work3/a174498/sst2_dataset/\"):\n",
    "    concept_path = os.path.join(concepts_path, name) + \"/\"\n",
    "    dataset = CustomIterableDataset(get_tensor_from_filename, concept_path)\n",
    "    concept_iter = dataset_to_dataloader(dataset)\n",
    "\n",
    "    return Concept(id=id, name=name, data_iter=concept_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_concept(name, id, concepts_path=\"data/tcav/text-sensitivity\"):\n",
    "    dataset = CustomIterableDataset(get_tensor_from_filename, concepts_path)\n",
    "    concept_iter = dataset_to_dataloader(dataset, batch_size=1)\n",
    "    return Concept(id=id, name=name, data_iter=concept_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcav.interpret(tokenized_test, experimental_sets=experimental_sets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('roberta_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c3ec90920587dcd62ca10f98568309ae5fe8dd1757bd16b3e1a83d20ad0c067"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
