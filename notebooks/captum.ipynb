{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer\n",
    "from datasets import load_from_disk, load_metric, Dataset, load_dataset\n",
    "\n",
    "#from captum.concept import TCAV, Concept\n",
    "from captum.concept._utils.data_iterator import dataset_to_dataloader, CustomIterableDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = {}\n",
    "A['taregt'] ={ 'c': {'lay': {'tcav':0}}, 'r':{'l':0}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'taregt': {'c': {'lay': {'tcav': 0}}, 'r': {'l': 0}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "A['taregt']['c']['lay'] = {'tcav':[1,2,3,4,4], 'acc' :123}\n",
    "A['taregt']['c']['lay2'] = {'tcav':[1,2,3,4,4], 'acc' :123}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'taregt': {'c': {'lay': {'tcav': [1, 2, 3, 4, 4], 'acc': 123},\n",
       "   'lay2': {'tcav': [1, 2, 3, 4, 4], 'acc': 123}},\n",
       "  'r': {'l': 0}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'taregt': {'c': {'cc': [1, 2, 3, 4, 4], 'bb': 123}, 't': {'a': 0}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAVS Random\n",
    "PATH_TO_Data = '/work3/s174498/concept_random_dataset/'\n",
    "\n",
    "PATH_random_cav = PATH_TO_Data+'cavs/'+'linear'+ '_classifier_on_layer_' + str(0)+'_random.pkl'\n",
    "\n",
    "with open(PATH_random_cav,'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "    cav_random = data['cavs']\n",
    "    acc_random = data['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_random_set = 100 #num_random_set\n",
    "counter_set='wikipedia_split'\n",
    "PATH_TO_Data = '/work3/s174498/concept_random_dataset/'\n",
    "\n",
    "\n",
    "num_ex_in_set = 150\n",
    "Data = counter_set #'wikipedia_split'\n",
    "\n",
    "file_name =  f'tensor_{Data}_on_{0}_layer_{num_random_set}_sets_with_{num_ex_in_set}' # f'tensor_{Data}_on_{layer_nr}_layer_{num_random_set}_sets_with_{num_ex_in_set}'\n",
    "file_random = PATH_TO_Data + '/'+ Data + '/' + file_name + '.pt'\n",
    "\n",
    "random_rep = torch.load(file_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150, 768)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = int(len(random_rep)/100)\n",
    "for i in range(98,100-1):\n",
    "    print(len(random_rep[i*N:(i+1)*N]))\n",
    "    \n",
    "random_rep[i*N:(i+1)*N].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_rep[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cav_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_grad = '/work3/s174498/sst2_dataset/grads_logits/'+'linear'+'_class_'+str(0)+'_layer_'+str(11)+'.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(PATH_grad,\u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m handle:\n\u001b[0;32m----> 2\u001b[0m     data \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(handle)\n\u001b[1;32m      3\u001b[0m grads \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mgrads\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m logits \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/storage.py:240\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_from_bytes\u001b[39m(b):\n\u001b[0;32m--> 240\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(io\u001b[39m.\u001b[39;49mBytesIO(b))\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/serialization.py:795\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    794\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 795\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/serialization.py:1012\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1010\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1011\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1012\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1014\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1016\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/serialization.py:958\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    954\u001b[0m     obj\u001b[39m.\u001b[39m_torch_load_uninitialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    955\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    956\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m    957\u001b[0m     deserialized_objects[root_key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m--> 958\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(obj, location),\n\u001b[1;32m    959\u001b[0m         dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    961\u001b[0m typed_storage \u001b[39m=\u001b[39m deserialized_objects[root_key]\n\u001b[1;32m    962\u001b[0m \u001b[39mif\u001b[39;00m view_metadata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/serialization.py:215\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 215\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    216\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "with open(PATH_grad,'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "grads = data['grads']\n",
    "logits = data['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cav_random\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "cav_random.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "768\n",
      "2\n",
      "768\n",
      "3\n",
      "768\n",
      "4\n",
      "768\n",
      "5\n",
      "768\n",
      "6\n",
      "768\n",
      "7\n",
      "768\n",
      "8\n",
      "768\n",
      "9\n",
      "768\n",
      "10\n",
      "768\n",
      "11\n",
      "768\n",
      "12\n",
      "768\n",
      "13\n",
      "768\n",
      "14\n",
      "768\n",
      "15\n",
      "768\n",
      "16\n",
      "768\n",
      "17\n",
      "768\n",
      "18\n",
      "768\n",
      "19\n",
      "768\n",
      "20\n",
      "768\n",
      "21\n",
      "768\n",
      "22\n",
      "768\n",
      "23\n",
      "768\n",
      "24\n",
      "768\n",
      "25\n",
      "768\n",
      "26\n",
      "768\n",
      "27\n",
      "768\n",
      "28\n",
      "768\n",
      "29\n",
      "768\n",
      "30\n",
      "768\n",
      "31\n",
      "768\n",
      "32\n",
      "768\n",
      "33\n",
      "768\n",
      "34\n",
      "768\n",
      "35\n",
      "768\n",
      "36\n",
      "768\n",
      "37\n",
      "768\n",
      "38\n",
      "768\n",
      "39\n",
      "768\n",
      "40\n",
      "768\n",
      "41\n",
      "768\n",
      "42\n",
      "768\n",
      "43\n",
      "768\n",
      "44\n",
      "768\n",
      "45\n",
      "768\n",
      "46\n",
      "768\n",
      "47\n",
      "768\n",
      "48\n",
      "768\n",
      "49\n",
      "768\n",
      "50\n",
      "768\n",
      "51\n",
      "768\n",
      "52\n",
      "768\n",
      "53\n",
      "768\n",
      "54\n",
      "768\n",
      "55\n",
      "768\n",
      "56\n",
      "768\n",
      "57\n",
      "768\n",
      "58\n",
      "768\n",
      "59\n",
      "768\n",
      "60\n",
      "768\n",
      "61\n",
      "768\n",
      "62\n",
      "768\n",
      "63\n",
      "768\n",
      "64\n",
      "768\n",
      "65\n",
      "768\n",
      "66\n",
      "768\n",
      "67\n",
      "768\n",
      "68\n",
      "768\n",
      "69\n",
      "768\n",
      "70\n",
      "768\n",
      "71\n",
      "768\n",
      "72\n",
      "768\n",
      "73\n",
      "768\n",
      "74\n",
      "768\n",
      "75\n",
      "768\n",
      "76\n",
      "768\n",
      "77\n",
      "768\n",
      "78\n",
      "768\n",
      "79\n",
      "768\n",
      "80\n",
      "768\n",
      "81\n",
      "768\n",
      "82\n",
      "768\n",
      "83\n",
      "768\n",
      "84\n",
      "768\n",
      "85\n",
      "768\n",
      "86\n",
      "768\n",
      "87\n",
      "768\n",
      "88\n",
      "768\n",
      "89\n",
      "768\n",
      "90\n",
      "768\n",
      "91\n",
      "768\n",
      "92\n",
      "768\n",
      "93\n",
      "768\n",
      "94\n",
      "768\n",
      "95\n",
      "768\n",
      "96\n",
      "768\n",
      "97\n",
      "768\n",
      "98\n",
      "768\n",
      "99\n",
      "768\n",
      "100\n",
      "768\n",
      "101\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for i in cav_random:\n",
    "    n += 1\n",
    "    print(n)\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,'/zhome/94/5/127021/speciale/master_project')\n",
    "from src.models.captum.tcav import TCAV\n",
    "from src.models.captum.concept import Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /work3/s174498/final/linear_head/checkpoint-1500 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = '/work3/s174498/final/linear_head/checkpoint-1500' #finetuning-sentiment-model-test-head3/checkpoint-500'\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(checkpoint,output_hidden_states = True,return_dict = True,)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer_checkpoint = RobertaTokenizer.from_pretrained(checkpoint) \n",
    "#tokenizer_pretrained = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "datadir = '/work3/s174498/sst2_dataset/'\n",
    "#test_0_dataset = load_from_disk(datadir + 'test_0_dataset')\n",
    "test_1_dataset = load_from_disk(datadir + 'test_1_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer_checkpoint.encode(test_0_dataset['sentence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.roberta.encoder.layer[0].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "#for i in range(len(test_1_dataset['sentence'])):\n",
    "#    j = len(test_1_dataset['sentence'][i])\n",
    "#    if j > N :\n",
    "#        N = j\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_from_filename(filename):\n",
    "    #datadir = '/work3/s174498/sst2_dataset/'\n",
    "    ds = load_from_disk(filename)\n",
    "    const_len = N\n",
    "    ds_text = ds['sentence']\n",
    "\n",
    "    for i in range(len(ds_text)):\n",
    "        #text_idx = tokenizer_checkpoint.tokenize(ds_text[i])\n",
    "        #text_idx[i] = tokenizer_checkpoint.encode(text_idx[i])\n",
    "        text_idx = tokenizer_checkpoint.encode(ds_text[i])\n",
    "        text_idx += [1] * max(0, const_len - len(text_idx[i]))\n",
    "        text_indices = torch.tensor(text_idx, device=device).unsqueeze(0)\n",
    "        yield text_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hent concept data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_concept(name, id, concepts_path=\"/work3/s174498/sst2_dataset/\"):\n",
    "    concept_path = os.path.join(concepts_path, name) + \"/\"\n",
    "    #print(concept_path)\n",
    "    dataset = CustomIterableDataset(get_tensor_from_filename, concept_path)\n",
    "    #print(dataset.)\n",
    "    concept_iter = dataset_to_dataloader(dataset, batch_size=1)\n",
    "    #print(concept_iter)\n",
    "\n",
    "    return Concept(id=id, name=name, data_iter=concept_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_concept = assemble_concept(\"positive\", 0, concepts_path=\"/work3/s174498/sst2_dataset/\")\n",
    "#pos_concept2 = assemble_concept(\"positive\", 2, concepts_path=\"/work3/s174498/sst2_dataset/\")\n",
    "\n",
    "neg_concept = assemble_concept(\"negative\", 5, concepts_path=\"/work3/s174498/sst2_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_sets=[[pos_concept, neg_concept]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input target text\n",
    "**Should be converted to tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = test_1_dataset['sentence'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covert_text_to_tensor(input_texts):\n",
    "    input_tensors = []\n",
    "    for input_text in input_texts:\n",
    "        const_len = N\n",
    "        #text_idx = tokenizer_checkpoint.tokenize(input_text)\n",
    "        text_idx = tokenizer_checkpoint.encode(input_text)\n",
    "        text_idx += [1] * max(0, const_len - len(text_idx))\n",
    "        input_tensor = torch.tensor(text_idx, device=device).unsqueeze(0)\n",
    "        input_tensors.append(input_tensor)\n",
    "    return torch.cat(input_tensors)\n",
    "\n",
    "input_text_indices = covert_text_to_tensor(input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/94/5/127021/miniconda3/envs/roberta_env/lib/python3.9/site-packages/captum/concept/_utils/classifier.py:130: UserWarning: Using default classifier for TCAV which keeps input both train and test datasets in the memory. Consider defining your own classifier that doesn't rely heavily on memory, for large number of concepts, by extending `Classifer` abstract class\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tcav = TCAV(model, layers=['roberta.encoder.layer.0.output.LayerNorm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> TEST 1 <<<\n",
      ">>> CAVs 1\n",
      ">>> CAVs 2.0\n",
      ">>> CAVs 2.1\n",
      ">>> gen.act. 1\n",
      ">>> GEN ACT 1\n",
      ">>> GEN ACT 2\n",
      ">>> GEN ACT 3\n",
      ">>> GEN ACT 4\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f3568e60460>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'generator'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tcav\u001b[39m.\u001b[39;49minterpret(input_text_indices , experimental_sets\u001b[39m=\u001b[39;49mexperimental_sets)\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/captum/log/__init__.py:35\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/speciale/master_project/src/models/captum/tcav.py:685\u001b[0m, in \u001b[0;36mTCAV.interpret\u001b[0;34m(self, inputs, experimental_sets, target, additional_forward_args, processes, **kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mattribute_to_layer_input\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs, (\n\u001b[1;32m    680\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPlease, set `attribute_to_layer_input` flag as a constructor \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    681\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39margument to TCAV class. In that case it will be applied \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mconsistently to both layer activation and layer attribution methods.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    683\u001b[0m )\n\u001b[1;32m    684\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>>> TEST 1 <<<\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 685\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_cavs(experimental_sets, processes\u001b[39m=\u001b[39;49mprocesses)\n\u001b[1;32m    686\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>>> TEST 2 <<<\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    687\u001b[0m scores: Dict[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Tensor]]] \u001b[39m=\u001b[39m defaultdict(\n\u001b[1;32m    688\u001b[0m     \u001b[39mlambda\u001b[39;00m: defaultdict()\n\u001b[1;32m    689\u001b[0m )\n",
      "File \u001b[0;32m~/speciale/master_project/src/models/captum/tcav.py:538\u001b[0m, in \u001b[0;36mTCAV.compute_cavs\u001b[0;34m(self, experimental_sets, force_train, processes)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>>> CAVs 2.1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    537\u001b[0m     \u001b[39m# Generate activations for missing (concept, layers)\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_activations(concept_layers)\n\u001b[1;32m    539\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>>> CAVs 2.2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    540\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/speciale/master_project/src/models/captum/tcav.py:406\u001b[0m, in \u001b[0;36mTCAV.generate_activations\u001b[0;34m(self, concept_layers)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[39mfor\u001b[39;00m concept \u001b[39min\u001b[39;00m concept_layers:\n\u001b[1;32m    405\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>>> gen.act. 1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 406\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_activation(concept_layers[concept], concept)\n",
      "File \u001b[0;32m~/speciale/master_project/src/models/captum/tcav.py:364\u001b[0m, in \u001b[0;36mTCAV.generate_activation\u001b[0;34m(self, layers, concept)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>>> GEN ACT 4\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    363\u001b[0m \u001b[39mprint\u001b[39m(concept\u001b[39m.\u001b[39mdata_iter)\n\u001b[0;32m--> 364\u001b[0m \u001b[39mfor\u001b[39;00m i, examples \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(concept\u001b[39m.\u001b[39mdata_iter):\n\u001b[1;32m    365\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mhej\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    366\u001b[0m     \u001b[39mprint\u001b[39m(examples)\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_iter)\n\u001b[0;32m---> 44\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:151\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m             \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    149\u001b[0m             \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[0;32m--> 151\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'generator'>"
     ]
    }
   ],
   "source": [
    "tcav.interpret(input_text_indices , experimental_sets=experimental_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pos_concept\u001b[39m.\u001b[39;49mdata_iter[\u001b[39m0\u001b[39;49m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "pos_concept.data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.roberta.encoder#.layer[0].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('roberta_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c3ec90920587dcd62ca10f98568309ae5fe8dd1757bd16b3e1a83d20ad0c067"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
