{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer\n",
    "from datasets import load_from_disk, load_metric, Dataset, load_dataset\n",
    "\n",
    "#from captum.concept import TCAV, Concept\n",
    "from captum.concept._utils.data_iterator import dataset_to_dataloader, CustomIterableDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,'/zhome/94/5/127021/speciale/master_project')\n",
    "from src.models.captum.tcav import TCAV\n",
    "from src.models.captum.concept import Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /work3/s174498/final/linear_head/checkpoint-1500 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = '/work3/s174498/final/linear_head/checkpoint-1500' #finetuning-sentiment-model-test-head3/checkpoint-500'\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(checkpoint,output_hidden_states = True,return_dict = True,)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer_checkpoint = RobertaTokenizer.from_pretrained(checkpoint) \n",
    "#tokenizer_pretrained = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "datadir = '/work3/s174498/sst2_dataset/'\n",
    "#test_0_dataset = load_from_disk(datadir + 'test_0_dataset')\n",
    "test_1_dataset = load_from_disk(datadir + 'test_1_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer_checkpoint.encode(test_0_dataset['sentence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.roberta.encoder.layer[0].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "#for i in range(len(test_1_dataset['sentence'])):\n",
    "#    j = len(test_1_dataset['sentence'][i])\n",
    "#    if j > N :\n",
    "#        N = j\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_from_filename(filename):\n",
    "    #datadir = '/work3/s174498/sst2_dataset/'\n",
    "    ds = load_from_disk(filename)\n",
    "    const_len = N\n",
    "    ds_text = ds['sentence']\n",
    "\n",
    "    for i in range(len(ds_text)):\n",
    "        #text_idx = tokenizer_checkpoint.tokenize(ds_text[i])\n",
    "        #text_idx[i] = tokenizer_checkpoint.encode(text_idx[i])\n",
    "        text_idx = tokenizer_checkpoint.encode(ds_text[i])\n",
    "        text_idx += [1] * max(0, const_len - len(text_idx[i]))\n",
    "        text_indices = torch.tensor(text_idx, device=device).unsqueeze(0)\n",
    "        yield text_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hent concept data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_concept(name, id, concepts_path=\"/work3/s174498/sst2_dataset/\"):\n",
    "    concept_path = os.path.join(concepts_path, name) + \"/\"\n",
    "    #print(concept_path)\n",
    "    dataset = CustomIterableDataset(get_tensor_from_filename, concept_path)\n",
    "    #print(dataset.)\n",
    "    concept_iter = dataset_to_dataloader(dataset, batch_size=1)\n",
    "    #print(concept_iter)\n",
    "\n",
    "    return Concept(id=id, name=name, data_iter=concept_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_concept = assemble_concept(\"positive\", 0, concepts_path=\"/work3/s174498/sst2_dataset/\")\n",
    "#pos_concept2 = assemble_concept(\"positive\", 2, concepts_path=\"/work3/s174498/sst2_dataset/\")\n",
    "\n",
    "neg_concept = assemble_concept(\"negative\", 5, concepts_path=\"/work3/s174498/sst2_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_sets=[[pos_concept, neg_concept]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input target text\n",
    "**Should be converted to tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = test_1_dataset['sentence'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covert_text_to_tensor(input_texts):\n",
    "    input_tensors = []\n",
    "    for input_text in input_texts:\n",
    "        const_len = N\n",
    "        #text_idx = tokenizer_checkpoint.tokenize(input_text)\n",
    "        text_idx = tokenizer_checkpoint.encode(input_text)\n",
    "        text_idx += [1] * max(0, const_len - len(text_idx))\n",
    "        input_tensor = torch.tensor(text_idx, device=device).unsqueeze(0)\n",
    "        input_tensors.append(input_tensor)\n",
    "    return torch.cat(input_tensors)\n",
    "\n",
    "input_text_indices = covert_text_to_tensor(input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/94/5/127021/miniconda3/envs/roberta_env/lib/python3.9/site-packages/captum/concept/_utils/classifier.py:130: UserWarning: Using default classifier for TCAV which keeps input both train and test datasets in the memory. Consider defining your own classifier that doesn't rely heavily on memory, for large number of concepts, by extending `Classifer` abstract class\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tcav = TCAV(model, layers=['roberta.encoder.layer.0.output.LayerNorm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> TEST 1 <<<\n",
      ">>> CAVs 1\n",
      ">>> CAVs 2.0\n",
      ">>> CAVs 2.1\n",
      ">>> gen.act. 1\n",
      ">>> GEN ACT 1\n",
      ">>> GEN ACT 2\n",
      ">>> GEN ACT 3\n",
      ">>> GEN ACT 4\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f3568e60460>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'generator'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tcav\u001b[39m.\u001b[39;49minterpret(input_text_indices , experimental_sets\u001b[39m=\u001b[39;49mexperimental_sets)\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/captum/log/__init__.py:35\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/speciale/master_project/src/models/captum/tcav.py:685\u001b[0m, in \u001b[0;36mTCAV.interpret\u001b[0;34m(self, inputs, experimental_sets, target, additional_forward_args, processes, **kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mattribute_to_layer_input\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs, (\n\u001b[1;32m    680\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPlease, set `attribute_to_layer_input` flag as a constructor \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    681\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39margument to TCAV class. In that case it will be applied \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mconsistently to both layer activation and layer attribution methods.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    683\u001b[0m )\n\u001b[1;32m    684\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>>> TEST 1 <<<\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 685\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_cavs(experimental_sets, processes\u001b[39m=\u001b[39;49mprocesses)\n\u001b[1;32m    686\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>>> TEST 2 <<<\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    687\u001b[0m scores: Dict[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Tensor]]] \u001b[39m=\u001b[39m defaultdict(\n\u001b[1;32m    688\u001b[0m     \u001b[39mlambda\u001b[39;00m: defaultdict()\n\u001b[1;32m    689\u001b[0m )\n",
      "File \u001b[0;32m~/speciale/master_project/src/models/captum/tcav.py:538\u001b[0m, in \u001b[0;36mTCAV.compute_cavs\u001b[0;34m(self, experimental_sets, force_train, processes)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>>> CAVs 2.1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    537\u001b[0m     \u001b[39m# Generate activations for missing (concept, layers)\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_activations(concept_layers)\n\u001b[1;32m    539\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>>> CAVs 2.2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    540\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/speciale/master_project/src/models/captum/tcav.py:406\u001b[0m, in \u001b[0;36mTCAV.generate_activations\u001b[0;34m(self, concept_layers)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[39mfor\u001b[39;00m concept \u001b[39min\u001b[39;00m concept_layers:\n\u001b[1;32m    405\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>>> gen.act. 1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 406\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_activation(concept_layers[concept], concept)\n",
      "File \u001b[0;32m~/speciale/master_project/src/models/captum/tcav.py:364\u001b[0m, in \u001b[0;36mTCAV.generate_activation\u001b[0;34m(self, layers, concept)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>>> GEN ACT 4\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    363\u001b[0m \u001b[39mprint\u001b[39m(concept\u001b[39m.\u001b[39mdata_iter)\n\u001b[0;32m--> 364\u001b[0m \u001b[39mfor\u001b[39;00m i, examples \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(concept\u001b[39m.\u001b[39mdata_iter):\n\u001b[1;32m    365\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mhej\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    366\u001b[0m     \u001b[39mprint\u001b[39m(examples)\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_iter)\n\u001b[0;32m---> 44\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:151\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m             \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    149\u001b[0m             \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[0;32m--> 151\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'generator'>"
     ]
    }
   ],
   "source": [
    "tcav.interpret(input_text_indices , experimental_sets=experimental_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pos_concept\u001b[39m.\u001b[39;49mdata_iter[\u001b[39m0\u001b[39;49m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "pos_concept.data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.roberta.encoder#.layer[0].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('roberta_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c3ec90920587dcd62ca10f98568309ae5fe8dd1757bd16b3e1a83d20ad0c067"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
