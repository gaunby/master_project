{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "sys.path.insert(0,'/zhome/94/5/127021/speciale/master_project')\n",
    "from src.models.tcav.TCAV import get_preds_tcavs\n",
    "\n",
    "random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCAV test \n",
    "This notebook is created to be able to test **TCAV.py** function\n",
    "\n",
    "### Should be moved to \n",
    "**src/models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################\n",
    "######## SET ALL PARAMETERS HERE ############\n",
    "#############################################\n",
    "\n",
    "#FILE_NAME = 'negative_news_layer_0_11' # name of saved file \n",
    "N = 300 # number of target examples \n",
    "M = 150 # number of concept examples\n",
    "\n",
    "num_random_set = 500 # number of runs/random folders\n",
    "\n",
    "#concepts = ['hate','irony','offensive'] # if not hate or news set variable later on \n",
    "concepts = ['woman'] # 'intersex','man','transsexual',\n",
    "#concepts = ['news','world','sport','business','science']\n",
    "\n",
    "target_nr = 1\n",
    "target_name = 'positive'\n",
    "############################################\n",
    "############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.encoder.layer.9.output.dense\n",
      "TCAV for layer: 9\n",
      "cavs concept are saved.\n",
      "number of concept cavs: 500\n",
      "cavs random are saved.\n",
      "number of cavs 500\n",
      "logits and grads are saved.\n",
      "number of concept cavs: 500\n",
      "sens shape: (500, 300)\n",
      "number tcavs concept: 500\n",
      "Accuracy over all:\n",
      "0.6832727272727273\n",
      "TCAV score for the concept: \n",
      "0.024326666666666663 0.02610687350956534\n",
      "senestivities random lenght:\n",
      "(500, 300)\n",
      "300\n",
      "number tcavs random: 500\n",
      "Accuracy over all:\n",
      "0.500989898989899\n",
      "TCAV score for the concept: \n",
      "0.5075266666666667 0.36198482834996765\n",
      "roberta.encoder.layer.10.output.dense\n",
      "TCAV for layer: 10\n",
      "calculating concept cavs...\n",
      "Number ex (in each class) to compute CAVs on: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/94/5/127021/speciale/master_project/src/models/tcav/Roberta_model_data.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/zhome/94/5/127021/speciale/master_project/src/models/tcav/Roberta_model_data.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cavs random are saved.\n",
      "number of cavs 501\n",
      "logits and grads are saved.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 160\u001b[0m\n\u001b[1;32m    158\u001b[0m nr \u001b[39m=\u001b[39m nr \u001b[39m+\u001b[39m \u001b[39m9\u001b[39m\n\u001b[1;32m    159\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTCAV for layer:\u001b[39m\u001b[39m'\u001b[39m, nr)\n\u001b[0;32m--> 160\u001b[0m _,_,TCAV, acc, _,TCAV_random, acc_random \u001b[39m=\u001b[39m get_preds_tcavs(classifier \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mlinear\u001b[39;49m\u001b[39m'\u001b[39;49m,model_layer\u001b[39m=\u001b[39;49mlayer,layer_nr \u001b[39m=\u001b[39;49mnr,\n\u001b[1;32m    161\u001b[0m                                 target_text \u001b[39m=\u001b[39;49m target_data, desired_class\u001b[39m=\u001b[39;49mtarget_nr,\n\u001b[1;32m    162\u001b[0m                                 counter_set \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mwikipedia_split\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    163\u001b[0m                                 concept_text \u001b[39m=\u001b[39;49m concept_data, concept_name\u001b[39m=\u001b[39;49m concept_name,\n\u001b[1;32m    164\u001b[0m                                 num_runs\u001b[39m=\u001b[39;49mnum_random_set)\n\u001b[1;32m    165\u001b[0m save_tcav[target_name][concept_name][layer] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mTCAV\u001b[39m\u001b[39m'\u001b[39m:TCAV, \u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m:acc}\n\u001b[1;32m    166\u001b[0m save_tcav[target_name][\u001b[39m'\u001b[39m\u001b[39mrandom\u001b[39m\u001b[39m'\u001b[39m][layer] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mTCAV\u001b[39m\u001b[39m'\u001b[39m:TCAV_random, \u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m:acc_random}\n",
      "File \u001b[0;32m~/speciale/master_project/src/models/tcav/TCAV.py:244\u001b[0m, in \u001b[0;36mget_preds_tcavs\u001b[0;34m(classifier, model_layer, layer_nr, target_text, desired_class, counter_set, concept_text, concept_name, num_runs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mlogits and grads are saved.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    243\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(PATH_grad,\u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m handle:\n\u001b[0;32m--> 244\u001b[0m   data \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(handle)\n\u001b[1;32m    245\u001b[0m grads \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mgrads\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    246\u001b[0m logits \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/storage.py:240\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_from_bytes\u001b[39m(b):\n\u001b[0;32m--> 240\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(io\u001b[39m.\u001b[39;49mBytesIO(b))\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/serialization.py:795\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    794\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 795\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/serialization.py:1012\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1010\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1011\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1012\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1014\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1016\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/serialization.py:958\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    954\u001b[0m     obj\u001b[39m.\u001b[39m_torch_load_uninitialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    955\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    956\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m    957\u001b[0m     deserialized_objects[root_key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m--> 958\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(obj, location),\n\u001b[1;32m    959\u001b[0m         dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    961\u001b[0m typed_storage \u001b[39m=\u001b[39m deserialized_objects[root_key]\n\u001b[1;32m    962\u001b[0m \u001b[39mif\u001b[39;00m view_metadata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/serialization.py:215\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 215\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    216\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# target data \n",
    "# load\n",
    "filename = \"/work3/s174498/sst2_dataset/positive\"\n",
    "ds_pos = load_from_disk(filename)\n",
    "ds_pos_text = ds_pos['sentence']\n",
    "\n",
    "filename = \"/work3/s174498/sst2_dataset/negative\"\n",
    "ds_neg = load_from_disk(filename)\n",
    "ds_neg_text = ds_neg['sentence']\n",
    "\n",
    "pos = [ds_pos_text[i] for i in list(np.random.choice(len(ds_pos_text),N))]\n",
    "neg = [ds_neg_text[i] for i in list(np.random.choice(len(ds_neg_text),N))]\n",
    "\n",
    "# Concept data \n",
    "# load hate\n",
    "datadir = '/work3/s174498/concept_random_dataset/'\n",
    "filename = 'tweet_hate/test'\n",
    "ds_hate = load_from_disk(datadir + filename)\n",
    "df_label_hate = pd.DataFrame(ds_hate['label'])\n",
    "idx_hate = df_label_hate[df_label_hate[0] == 1].index.values\n",
    "ds_hate = ds_hate['text']\n",
    "hate = [ds_hate[i] for i in list(np.random.choice( idx_hate,M))]\n",
    "\n",
    "# load offensive\n",
    "filename = 'tweet_offensive/test'\n",
    "ds_off = load_from_disk(datadir + filename)\n",
    "df_label_off = pd.DataFrame(ds_off['label'])\n",
    "idx_off = df_label_off[df_label_off[0] == 1].index.values\n",
    "ds_off = ds_off['text']\n",
    "offen = [ds_off[i] for i in list(np.random.choice( idx_off,M))]\n",
    "\n",
    "# load irony \n",
    "filename = 'tweet_irony/test'\n",
    "ds_irony = load_from_disk(datadir + filename)\n",
    "df_label_irony = pd.DataFrame(ds_irony['label'])\n",
    "idx_irony = df_label_irony[df_label_irony[0] == 1].index.values\n",
    "ds_irony = ds_irony['text']\n",
    "irony = [ds_irony[i] for i in list(np.random.choice( idx_irony,M))]\n",
    "\n",
    "# load woman \n",
    "filefolder = 'wikipedia_20220301/gender_concepts/'\n",
    "filename = 'woman_female'\n",
    "ds_woman = load_from_disk(datadir +filefolder + filename)\n",
    "ds_woman = ds_woman['text_list']\n",
    "woman = [ds_woman[i] for i in list(np.random.choice(len(ds_woman),M))]\n",
    "\n",
    "# load man\n",
    "filename = 'man_male'\n",
    "ds_man = load_from_disk(datadir +filefolder + filename)\n",
    "ds_man = ds_man['text_list']\n",
    "man = [ds_man[i] for i in list(np.random.choice(len(ds_man),M))]\n",
    "\n",
    "# load trans\n",
    "filename = 'Transsexual'\n",
    "ds_trans = load_from_disk(datadir +filefolder + filename)\n",
    "ds_trans = ds_trans['text_list']\n",
    "trans = [ds_trans[i] for i in list(np.random.choice(len(ds_trans),M))]\n",
    "\n",
    "# load intersex\n",
    "filename = 'Intersex'\n",
    "ds_inter = load_from_disk(datadir +filefolder + filename)\n",
    "ds_inter = ds_inter['text_list']\n",
    "inter = [ds_inter[i] for i in list(np.random.choice(len(ds_inter),M))]\n",
    "\n",
    "\n",
    "# load 20 newsgroups\n",
    "filename = '20_newsgroups/test'\n",
    "ds_news= load_from_disk(datadir + filename)\n",
    "ds_news = ds_news['text']\n",
    "news = [ds_news[i] for i in list(np.random.choice(len(ds_news),M))]\n",
    "\n",
    "# load ag news \n",
    "# labels: World (0), Sports (1), Business (2), Sci/Tech (3).\n",
    "filename = 'ag_news/test'\n",
    "ag_news= load_from_disk(datadir + filename)\n",
    "\n",
    "df_label_ag = pd.DataFrame(ag_news['label'])\n",
    "idx_world = df_label_ag[df_label_ag[0] == 0].index.values\n",
    "idx_sport = df_label_ag[df_label_ag[0] == 1].index.values\n",
    "idx_buss = df_label_ag[df_label_ag[0] == 2].index.values\n",
    "idx_sci = df_label_ag[df_label_ag[0] == 3].index.values\n",
    "\n",
    "ag_news = ag_news['text']\n",
    "ag_world = [ag_news[i] for i in list(np.random.choice( idx_world,M))]\n",
    "ag_sport = [ag_news[i] for i in list(np.random.choice( idx_sport,M))]\n",
    "ag_buss = [ag_news[i] for i in list(np.random.choice( idx_buss,M))]\n",
    "ag_sci = [ag_news[i] for i in list(np.random.choice( idx_sci,M))]\n",
    "ag_news = [ag_news[i] for i in list(np.random.choice(len(ag_news),M))]\n",
    "\n",
    "\n",
    "layers = [#'roberta.encoder.layer.0.output.dense',\n",
    "        #'roberta.encoder.layer.1.output.dense',\n",
    "        #'roberta.encoder.layer.2.output.dense',\n",
    "        #'roberta.encoder.layer.3.output.dense',\n",
    "        #'roberta.encoder.layer.4.output.dense',\n",
    "        #'roberta.encoder.layer.5.output.dense',\n",
    "        #'roberta.encoder.layer.6.output.dense',\n",
    "        #'roberta.encoder.layer.7.output.dense',\n",
    "        #'roberta.encoder.layer.8.output.dense',\n",
    "        'roberta.encoder.layer.9.output.dense',\n",
    "        'roberta.encoder.layer.10.output.dense',\n",
    "        'roberta.encoder.layer.11.output.dense']\n",
    "\n",
    "\n",
    "if target_name == 'negative':\n",
    "    target_data = neg\n",
    "elif target_name == 'positive':\n",
    "    target_data = pos\n",
    "else:\n",
    "    print('wrong target data name')\n",
    "\n",
    "\n",
    "# TCAV data \n",
    "save_tcav = {}\n",
    "save_tcav[target_name] = {concepts[0]:{layers[0] :{'TCAV':0 ,'acc':0}}, 'random':{layers[0]:{'TCAV':0}}}\n",
    "for concept_name in concepts:\n",
    "    if concept_name == 'hate':\n",
    "        concept_data = hate #\n",
    "        save_tcav[target_name][concept_name] = {layers[0] :{'TCAV':0 ,'acc':0}}\n",
    "    elif concept_name == 'offensive':\n",
    "        concept_data = offen #\n",
    "        save_tcav[target_name][concept_name] = {layers[0] :{'TCAV':0 ,'acc':0}}\n",
    "    elif concept_name == 'irony':\n",
    "        concept_data = irony #\n",
    "        save_tcav[target_name][concept_name] = {layers[0] :{'TCAV':0 ,'acc':0}}\n",
    "    elif concept_name == 'news':\n",
    "        concept_data = ag_news #\n",
    "        save_tcav[target_name][concept_name] = {layers[0] :{'TCAV':0 ,'acc':0}}\n",
    "    elif concept_name == 'sport':\n",
    "        concept_data = ag_sport #\n",
    "        save_tcav[target_name][concept_name] = {layers[0] :{'TCAV':0 ,'acc':0}}\n",
    "    elif concept_name == 'business':\n",
    "        concept_data = ag_buss #\n",
    "        save_tcav[target_name][concept_name] = {layers[0] :{'TCAV':0 ,'acc':0}}\n",
    "    elif concept_name == 'world':\n",
    "        concept_data = ag_world #\n",
    "        save_tcav[target_name][concept_name] = {layers[0] :{'TCAV':0 ,'acc':0}}\n",
    "    elif concept_name == 'science':\n",
    "        concept_data = ag_sci #\n",
    "        save_tcav[target_name][concept_name] = {layers[0] :{'TCAV':0 ,'acc':0}}\n",
    "    elif concept_name == 'woman':\n",
    "        concept_data = woman\n",
    "        save_tcav[target_name][concept_name] = {layers[0] :{'TCAV':0 ,'acc':0}}\n",
    "    elif concept_name == 'man':\n",
    "        concept_data = man\n",
    "        save_tcav[target_name][concept_name] = {layers[0] :{'TCAV':0 ,'acc':0}}\n",
    "    elif concept_name == 'intersex':\n",
    "        concept_data = inter\n",
    "        save_tcav[target_name][concept_name] = {layers[0] :{'TCAV':0 ,'acc':0}}\n",
    "    elif concept_name == 'transsexual':\n",
    "        concept_data = trans\n",
    "        save_tcav[target_name][concept_name] = {layers[0] :{'TCAV':0 ,'acc':0}}\n",
    "    else:\n",
    "        print('missing concet data name')\n",
    "\n",
    "    for nr, layer in enumerate(layers[:2]):\n",
    "        print(layer)\n",
    "        nr = nr + 9\n",
    "        print('TCAV for layer:', nr)\n",
    "        _,_,TCAV, acc, _,TCAV_random, acc_random = get_preds_tcavs(classifier = 'linear',model_layer=layer,layer_nr =nr,\n",
    "                                        target_text = target_data, desired_class=target_nr,\n",
    "                                        counter_set = 'wikipedia_split',\n",
    "                                        concept_text = concept_data, concept_name= concept_name,\n",
    "                                        num_runs=num_random_set)\n",
    "        save_tcav[target_name][concept_name][layer] = {'TCAV':TCAV, 'acc':acc}\n",
    "        save_tcav[target_name]['random'][layer] = {'TCAV':TCAV_random, 'acc':acc_random}\n",
    "\n",
    "# saving the file \n",
    "#PATH =  f\"/work3/s174498/nlp_tcav_results/{FILE_NAME}.pkl\"\n",
    "#f = open(PATH ,\"wb\")\n",
    "#pickle.dump(save_tcav, f)\n",
    "#f.close()\n",
    "\n",
    "#print('FINISH')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cavs concept are saved.\n",
      "number of concept cavs: 501\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_Data = '/work3/s174498/concept_random_dataset/'\n",
    "\n",
    "PATH_concept_cav = PATH_TO_Data+'cavs/concept/'+concept_name+'_linear_classifier_on_layer_' + str(nr)+'_with_'+str(num_random_set)+'random.pkl'\n",
    "print('cavs concept are saved.')\n",
    "with open(PATH_concept_cav,'rb') as handle:\n",
    "  data = pickle.load(handle)\n",
    "concept_cavs = data['cavs']\n",
    "acc = data['acc']\n",
    "print('number of concept cavs:',len(concept_cavs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('roberta_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c3ec90920587dcd62ca10f98568309ae5fe8dd1757bd16b3e1a83d20ad0c067"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
