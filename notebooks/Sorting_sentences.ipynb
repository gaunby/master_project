{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(0,'/zhome/a6/6/127219/Speciale/master_project')\n",
    "from src.models.tcav.TCAV import get_preds_tcavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def (classifier = 'linear',model_layer = \"roberta.encoder.layer.11.output.dense\", layer_nr = '11',target_text = random_text,desired_class = 0,counter_set = 'wikipedia_split',concept_text = random_text, num_runs = 10):\n",
    "  \n",
    "  num_runs = 10\n",
    "  num_random_set = num_runs #num_random_set\n",
    "\n",
    "  # load tokenizer \n",
    "  classifier = 'linear'\n",
    "\n",
    "  if classifier == 'linear':\n",
    "    folder = '/work3/s174498/final/linear_head/checkpoint-1500'\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(folder)\n",
    "  elif classifier == 'original':\n",
    "    folder = '/work3/s174498/final/original_head/checkpoint-500'\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(folder)\n",
    "  \n",
    "  model_layer = \"roberta.encoder.layer.11.output.dense\"\n",
    "  model = RobertaClassifier(model_type = classifier, model_layer = model_layer )\n",
    "  \n",
    "  layer_nr = '11'\n",
    "  concept_text = \n",
    "\n",
    "  if counter_set=='wikipedia_split':\n",
    "    \n",
    "    num_ex_in_set = len(concept_text)\n",
    "    Data = counter_set #'wikipedia_split'\n",
    "    \n",
    "    file_name =  f'tensor_{Data}_on_{layer_nr}_layer_{num_random_set}_sets_with_{num_ex_in_set}' # f'tensor_{Data}_on_{layer_nr}_layer_{num_random_set}_sets_with_{num_ex_in_set}'\n",
    "    file_random = PATH_TO_Data + '/'+ Data + '/' + file_name + '.pt'\n",
    "\n",
    "    if os.path.exists(file_random):\n",
    "      random_rep = torch.load(file_random)\n",
    "    else:\n",
    "      print('Counter part does not have a representation for this model layer or does not have the correct size.\\nCreate by running: embedding_layer_rep.py')\n",
    "      return\n",
    "\n",
    "  else:\n",
    "    print('Counter part does not have a representation for this random dataset\\nCreate by running: embedding_layer_rep.py')\n",
    "    return\n",
    "\n",
    "  print('calculating concept cavs...')\n",
    "  model.to(device)\n",
    "  concept_cavs, acc = compute_cavs(model,tokenizer, concept_text, random_rep, num_runs=num_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_tcavs(classifier = 'linear',model_layer = \"roberta.encoder.layer.11.output.dense\", layer_nr = '11',target_text = random_text,desired_class = 0,counter_set = 'wikipedia_split',concept_text = random_text, num_runs = 10):\n",
    "  #returns logits, sensitivies and tcav score\n",
    "  num_random_set = num_runs #num_random_set\n",
    "  # load tokenizer \n",
    "  if classifier == 'linear':\n",
    "    folder = '/work3/s174498/final/linear_head/checkpoint-1500'\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(folder)\n",
    "  elif classifier == 'original':\n",
    "    folder = '/work3/s174498/final/original_head/checkpoint-500'\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(folder)\n",
    "  else:\n",
    "    print('model is unknown')\n",
    "    return \n",
    "  \n",
    "  model = RobertaClassifier(model_type = classifier, model_layer = model_layer )\n",
    "  \n",
    "  if len(concept_text) < 100:\n",
    "    print('Too few concept text examples. Must be greater than 100')\n",
    "    return\n",
    "\n",
    "  if counter_set=='wikipedia_split':\n",
    "    \n",
    "    num_ex_in_set = len(concept_text)\n",
    "    Data = counter_set #'wikipedia_split'\n",
    "    \n",
    "    file_name =  f'tensor_{Data}_on_{layer_nr}_layer_{num_random_set}_sets_with_{num_ex_in_set}' # f'tensor_{Data}_on_{layer_nr}_layer_{num_random_set}_sets_with_{num_ex_in_set}'\n",
    "    file_random = PATH_TO_Data + '/'+ Data + '/' + file_name + '.pt'\n",
    "\n",
    "    if os.path.exists(file_random):\n",
    "      random_rep = torch.load(file_random)\n",
    "    else:\n",
    "      print('Counter part does not have a representation for this model layer or does not have the correct size.\\nCreate by running: embedding_layer_rep.py')\n",
    "      return\n",
    "\n",
    "  else:\n",
    "    print('Counter part does not have a representation for this random dataset\\nCreate by running: embedding_layer_rep.py')\n",
    "    return\n",
    "\n",
    "  print('calculating concept cavs...')\n",
    "  model.to(device)\n",
    "  concept_cavs, acc = compute_cavs(model,tokenizer, concept_text, random_rep, num_runs=num_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#from src/models/run_tcav.py\n",
    "_,_,TCAV, acc, _,TCAV_random = get_preds_tcavs(classifier = 'linear',model_layer=layer,layer_nr =nr,\n",
    "                                        target_text = target_data, desired_class=target_nr,\n",
    "                                        counter_set = 'wikipedia_split',\n",
    "                                        concept_text = concept_data, \n",
    "                                        num_runs=num_random_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('sent_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9738a1daac20355f138ac5f9490b2e4e7176effdeaa1fb550cd3c1d19d286cfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
