{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5CYHcPO0AhL"
      },
      "source": [
        "# Visualize your ü§ó Hugging Face data\n",
        "#### üõ†Ô∏è Installation and set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JurK8mIH0AhP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import RobertaTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVXUoDKy0AhQ"
      },
      "source": [
        "### üõ´ Data and model preparation\n",
        "#### üè∑Ô∏è Loading a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "quwoNjNt0AhR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/zhome/94/5/127021/miniconda3/envs/roberta_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Found cached dataset sst2 (/zhome/94/5/127021/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 489.91it/s]\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"sst2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['idx', 'sentence', 'label'],\n",
              "        num_rows: 67349\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['idx', 'sentence', 'label'],\n",
              "        num_rows: 872\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['idx', 'sentence', 'label'],\n",
              "        num_rows: 1821\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For demo sub-sample dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aM29Dj760AhX"
      },
      "outputs": [],
      "source": [
        "small_data_train = dataset['train'].select(range(dataset['train'].num_rows // 10))\n",
        "# alternative methods\n",
        "# dataset[\"train\"].shuffle(seed=42).select([i for i in list(range(100))])\n",
        "small_data_val = dataset['validation'].select(range(dataset['validation'].num_rows // 10)) # dataset[\"validation\"].shuffle(seed=42).select([i for i in list(range(50))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9lYU6nB0AhX"
      },
      "source": [
        "### ‚öôÔ∏è Tokenizing the dataset\n",
        "In a typical NLP workflow, we must first tokenize our dataset.\n",
        "\n",
        "Converting the stream of characters in the text into a stream of defined \"tokens\", which can be anything from a smaller set of characters to words from a vocabulary.\n",
        "\n",
        "We will use a pretrained model, so we inherit its tokenization scheme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 899k/899k [00:00<00:00, 1.42MB/s] \n",
            "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456k/456k [00:00<00:00, 863kB/s] \n",
            "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 481/481 [00:00<00:00, 378kB/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hide new secretions from the parental units ',\n",
              " 'contains no wit , only labored gags ',\n",
              " 'that loves its characters and communicates something rather beautiful about human nature ',\n",
              " 'remains utterly satisfied to remain the same throughout ',\n",
              " 'on the worst revenge-of-the-nerds clich√©s the filmmakers could dredge up ']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "small_data_train['sentence'][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_text = ['hello world',' hello world', 'hello world.','hello world?',' hello world ','hello world .']\n",
        "test_text2 = ['in store and dog',' in store ','in store ',' in store','in store?','in store.','in store .']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [[0, 179, 1400, 8, 2335, 2], [0, 11, 1400, 1437, 2], [0, 179, 1400, 1437, 2], [0, 11, 1400, 2], [0, 179, 1400, 116, 2], [0, 179, 1400, 4, 2], [0, 179, 1400, 479, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(test_text2, truncation = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [[0, 37265, 92, 3556, 2485, 31, 5, 20536, 2833, 1437, 2], [0, 10800, 5069, 117, 22094, 2156, 129, 6348, 3995, 821, 8299, 1437, 2], [0, 6025, 6138, 63, 3768, 8, 39906, 402, 1195, 2721, 59, 1050, 2574, 1437, 2], [0, 5593, 5069, 19223, 10028, 7, 1091, 5, 276, 1328, 1437, 2], [0, 261, 5, 2373, 13543, 12, 1116, 12, 627, 12, 1396, 11622, 43848, 5739, 5, 17504, 115, 31120, 1899, 62, 1437, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(small_data_train['sentence'][:5])#, truncation = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPYLcMM00AhZ"
      },
      "source": [
        "We then map the tokenizer over our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n",
            "Downloading and preparing dataset sst2/default (download: 7.09 MiB, generated: 4.78 MiB, post-processed: Unknown size, total: 11.88 MiB) to /zhome/94/5/127021/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5...\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "[Errno 28] No space left on device: '/zhome/94/5/127021/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5.incomplete'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_function\u001b[39m(examples):\n\u001b[1;32m      2\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(examples[\u001b[39m\"\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m\"\u001b[39m], truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39msst2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m small_train_dataset \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshuffle(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\u001b[39m.\u001b[39mselect([i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m))])\n\u001b[1;32m      7\u001b[0m small_val_dataset \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshuffle(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\u001b[39m.\u001b[39mselect([i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m))])\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/datasets/load.py:1742\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1739\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1741\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1742\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   1743\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1744\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1745\u001b[0m     ignore_verifications\u001b[39m=\u001b[39;49mignore_verifications,\n\u001b[1;32m   1746\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   1747\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1748\u001b[0m )\n\u001b[1;32m   1750\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   1752\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   1753\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/datasets/builder.py:793\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_manual_download(dl_manager)\n\u001b[1;32m    792\u001b[0m \u001b[39m# Create a tmp dir and rename to self._output_dir on successful exit.\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m \u001b[39mwith\u001b[39;00m incomplete_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir) \u001b[39mas\u001b[39;00m tmp_output_dir:\n\u001b[1;32m    794\u001b[0m     \u001b[39m# Temporarily assign _output_dir to tmp_data_dir to avoid having to forward\u001b[39;00m\n\u001b[1;32m    795\u001b[0m     \u001b[39m# it to every sub function.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m     \u001b[39mwith\u001b[39;00m temporary_assignment(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_output_dir\u001b[39m\u001b[39m\"\u001b[39m, tmp_output_dir):\n\u001b[1;32m    797\u001b[0m \n\u001b[1;32m    798\u001b[0m         \u001b[39m# Try to download the already prepared dataset files\u001b[39;00m\n\u001b[1;32m    799\u001b[0m         downloaded_from_gcs \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    120\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/datasets/builder.py:763\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare.<locals>.incomplete_dir\u001b[0;34m(dirname)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     tmp_dir \u001b[39m=\u001b[39m dirname \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.incomplete\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 763\u001b[0m     os\u001b[39m.\u001b[39;49mmakedirs(tmp_dir, exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    764\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m         \u001b[39myield\u001b[39;00m tmp_dir\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     mkdir(name, mode)\n\u001b[1;32m    226\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '/zhome/94/5/127021/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5.incomplete'"
          ]
        }
      ],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], truncation=True)\n",
        "\n",
        "dataset = load_dataset(\"sst2\")\n",
        "\n",
        "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select([i for i in list(range(100))])\n",
        "small_val_dataset = dataset[\"validation\"].shuffle(seed=42).select([i for i in list(range(50))])\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_val = small_val_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "#model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('roberta_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2c3ec90920587dcd62ca10f98568309ae5fe8dd1757bd16b3e1a83d20ad0c067"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
