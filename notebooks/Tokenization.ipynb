{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5CYHcPO0AhL"
      },
      "source": [
        "# Visualize your ü§ó Hugging Face data\n",
        "#### üõ†Ô∏è Installation and set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "JurK8mIH0AhP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer\n",
        "import torch\n",
        "from datasets import load_from_disk, load_metric, Dataset, load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVXUoDKy0AhQ"
      },
      "source": [
        "### üõ´ Data and model preparation\n",
        "#### üè∑Ô∏è Loading a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "quwoNjNt0AhR"
      },
      "outputs": [],
      "source": [
        "# load\n",
        "datadir = '/work3/s174498/sst2_dataset/'\n",
        "\n",
        "from_disk = True\n",
        "all = False\n",
        "\n",
        "if from_disk:\n",
        "    test_dataset = load_from_disk(datadir + 'test_dataset')\n",
        "    if all:\n",
        "        train_dataset = load_from_disk(datadir + 'train_dataset')\n",
        "        validation_dataset = load_from_disk(datadir + 'validation_dataset')\n",
        "else:\n",
        "    if all:\n",
        "        dataset = load_dataset(\"sst2\")\n",
        "        train_dataset = dataset['train']\n",
        "        validation_dataset = dataset['validation']\n",
        "        test_dataset = dataset['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9lYU6nB0AhX"
      },
      "source": [
        "### ‚öôÔ∏è Tokenizing the dataset\n",
        "In a typical NLP workflow, we must first tokenize our dataset.\n",
        "\n",
        "Converting the stream of characters in the text into a stream of defined \"tokens\", which can be anything from a smaller set of characters to words from a vocabulary.\n",
        "\n",
        "We will use a pretrained model, so we inherit its tokenization scheme.\n",
        "\n",
        "Wanting to see all files on RoBERTa e.g. tokenization https://huggingface.co/roberta-base/tree/main \n",
        "\n",
        "**Merge**-file explanation https://github.com/huggingface/transformers/issues/4777 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Files used for the RoBERTa pre-trained Tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current wokring directory /zhome/94/5/127021/speciale/master_project/notebooks\n"
          ]
        }
      ],
      "source": [
        "ellen_little_nb_path = '/zhome/94/5/127021/speciale/master_project/notebooks'\n",
        "roberta_files_path = '/work3/s174498/roberta_files/'\n",
        "print('Current wokring directory',os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load\n",
        "checkpoint = '/work3/s174498/finetuning-sentiment-model-all-samples-test6/checkpoint-1000'\n",
        "\n",
        "# tokenizer\n",
        "tokenizer_checkpoint = RobertaTokenizer.from_pretrained(checkpoint) \n",
        "tokenizer_pretrained = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# model\n",
        "model = RobertaForSequenceClassification.from_pretrained(checkpoint,output_hidden_states = True,return_dict = True)# output_attentions = True, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# merges files\n",
        "df_merges = pd.read_csv(\"/work3/s174498/roberta_files/merges.txt\", sep=\" \",  on_bad_lines='skip')\n",
        "\n",
        "# dict file\n",
        "df_dict = pd.read_csv(\"/work3/s174498/roberta_files/dict.txt\", sep=\" \", header = None, names = ['id_GPT_2','occurrence'])\n",
        "\n",
        "# tokenizer file\n",
        "file = open('/work3/s174498/roberta_files/tokenizer.json')\n",
        "tokenizer_json = json.load(file)\n",
        "file.close()\n",
        "\n",
        "# vocab file\n",
        "file = open('/work3/s174498/roberta_files/vocab.json')\n",
        "vocab = json.load(file)\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_GPT_2</th>\n",
              "      <th>occurrence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13</td>\n",
              "      <td>850314647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>262</td>\n",
              "      <td>800385005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "      <td>800251374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>284</td>\n",
              "      <td>432911125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>290</td>\n",
              "      <td>394899794</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  id_GPT_2  occurrence\n",
              "0       13   850314647\n",
              "1      262   800385005\n",
              "2       11   800251374\n",
              "3      284   432911125\n",
              "4      290   394899794"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_dict.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Vocab** consists of 50265 'units'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab length: 50265\n",
            "the 10 first: ['<s>', '<pad>', '</s>', '<unk>', '.', 'ƒ†the', ',', 'ƒ†to', 'ƒ†and', 'ƒ†of']\n"
          ]
        }
      ],
      "source": [
        "print('vocab length:', len(vocab.keys()))\n",
        "print('the 10 first:',list(vocab.keys())[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tokenizer** has a lot of information about model, and which setting are chosen and the vocab can be found here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'version': '1.0',\n",
              " 'truncation': None,\n",
              " 'padding': None,\n",
              " 'added_tokens': [{'id': 0,\n",
              "   'special': True,\n",
              "   'content': '<s>',\n",
              "   'single_word': False,\n",
              "   'lstrip': False,\n",
              "   'rstrip': False,\n",
              "   'normalized': True},\n",
              "  {'id': 1,\n",
              "   'special': True,\n",
              "   'content': '<pad>',\n",
              "   'single_word': False,\n",
              "   'lstrip': False,\n",
              "   'rstrip': False,\n",
              "   'normalized': True},\n",
              "  {'id': 2,\n",
              "   'special': True,\n",
              "   'content': '</s>',\n",
              "   'single_word': False,\n",
              "   'lstrip': False,\n",
              "   'rstrip': False,\n",
              "   'normalized': True},\n",
              "  {'id': 3,\n",
              "   'special': True,\n",
              "   'content': '<unk>',\n",
              "   'single_word': False,\n",
              "   'lstrip': False,\n",
              "   'rstrip': False,\n",
              "   'normalized': True},\n",
              "  {'id': 50264,\n",
              "   'special': True,\n",
              "   'content': '<mask>',\n",
              "   'single_word': False,\n",
              "   'lstrip': True,\n",
              "   'rstrip': False,\n",
              "   'normalized': True}],\n",
              " 'normalizer': None,\n",
              " 'pre_tokenizer': {'type': 'ByteLevel',\n",
              "  'add_prefix_space': False,\n",
              "  'trim_offsets': True},\n",
              " 'post_processor': {'type': 'RobertaProcessing',\n",
              "  'sep': ['</s>', 2],\n",
              "  'cls': ['<s>', 0],\n",
              "  'trim_offsets': True,\n",
              "  'add_prefix_space': False},\n",
              " 'decoder': {'type': 'ByteLevel',\n",
              "  'add_prefix_space': True,\n",
              "  'trim_offsets': True},\n",
              " 'model': {'dropout': None,\n",
              "  'unk_token': None,\n",
              "  'continuing_subword_prefix': '',\n",
              "  'end_of_word_suffix': '',\n",
              "  'fuse_unk': False,\n",
              "  'vocab': {'<s>': 0,\n",
              "   '<pad>': 1,\n",
              "   '</s>': 2,\n",
              "   '<unk>': 3,\n",
              "   '.': 4,\n",
              "   'ƒ†the': 5,\n",
              "   ',': 6,\n",
              "   'ƒ†to': 7,\n",
              "   'ƒ†and': 8,\n",
              "   'ƒ†of': 9,\n",
              "   'ƒ†a': 10,\n",
              "   'ƒ†in': 11,\n",
              "   '-': 12,\n",
              "   'ƒ†for': 13,\n",
              "   'ƒ†that': 14,\n",
              "   'ƒ†on': 15,\n",
              "   'ƒ†is': 16,\n",
              "   '√¢ƒ¢': 17,\n",
              "   \"'s\": 18,\n",
              "   'ƒ†with': 19,\n",
              "   'ƒ†The': 20,\n",
              "   'ƒ†was': 21,\n",
              "   'ƒ†\"': 22,\n",
              "   'ƒ†at': 23,\n",
              "   'ƒ†it': 24,\n",
              "   'ƒ†as': 25,\n",
              "   'ƒ†said': 26,\n",
              "   'ƒª': 27,\n",
              "   'ƒ†be': 28,\n",
              "   's': 29,\n",
              "   'ƒ†by': 30,\n",
              "   'ƒ†from': 31,\n",
              "   'ƒ†are': 32,\n",
              "   'ƒ†have': 33,\n",
              "   'ƒ†has': 34,\n",
              "   ':': 35,\n",
              "   'ƒ†(': 36,\n",
              "   'ƒ†he': 37,\n",
              "   'ƒ†I': 38,\n",
              "   'ƒ†his': 39,\n",
              "   'ƒ†will': 40,\n",
              "   'ƒ†an': 41,\n",
              "   'ƒ†this': 42,\n",
              "   ')': 43,\n",
              "   'ƒ†√¢ƒ¢': 44,\n",
              "   'ƒ†not': 45,\n",
              "   'ƒø': 46,\n",
              "   'ƒ†you': 47,\n",
              "   'ƒæ': 48,\n",
              "   'ƒ†their': 49,\n",
              "   'ƒ†or': 50,\n",
              "   'ƒ†they': 51,\n",
              "   'ƒ†we': 52,\n",
              "   'ƒ†but': 53,\n",
              "   'ƒ†who': 54,\n",
              "   'ƒ†more': 55,\n",
              "   'ƒ†had': 56,\n",
              "   'ƒ†been': 57,\n",
              "   'ƒ†were': 58,\n",
              "   'ƒ†about': 59,\n",
              "   ',\"': 60,\n",
              "   'ƒ†which': 61,\n",
              "   'ƒ†up': 62,\n",
              "   'ƒ†its': 63,\n",
              "   'ƒ†can': 64,\n",
              "   'ƒ†one': 65,\n",
              "   'ƒ†out': 66,\n",
              "   'ƒ†also': 67,\n",
              "   'ƒ†$': 68,\n",
              "   'ƒ†her': 69,\n",
              "   'ƒ†all': 70,\n",
              "   'ƒ†after': 71,\n",
              "   '.\"': 72,\n",
              "   '/': 73,\n",
              "   'ƒ†would': 74,\n",
              "   \"'t\": 75,\n",
              "   'ƒ†year': 76,\n",
              "   'ƒ†when': 77,\n",
              "   'ƒ†first': 78,\n",
              "   'ƒ†she': 79,\n",
              "   'ƒ†two': 80,\n",
              "   'ƒ†over': 81,\n",
              "   'ƒ†people': 82,\n",
              "   'ƒ†A': 83,\n",
              "   'ƒ†our': 84,\n",
              "   'ƒ†It': 85,\n",
              "   'ƒ†time': 86,\n",
              "   'ƒ†than': 87,\n",
              "   'ƒ†into': 88,\n",
              "   'ƒ†there': 89,\n",
              "   't': 90,\n",
              "   'ƒ†He': 91,\n",
              "   'ƒ†new': 92,\n",
              "   'ƒ†√¢ƒ¢ƒ∂': 93,\n",
              "   'ƒ†last': 94,\n",
              "   'ƒ†just': 95,\n",
              "   'ƒ†In': 96,\n",
              "   'ƒ†other': 97,\n",
              "   'ƒ†so': 98,\n",
              "   'ƒ†what': 99,\n",
              "   'I': 100,\n",
              "   'ƒ†like': 101,\n",
              "   'a': 102,\n",
              "   'ƒ†some': 103,\n",
              "   'S': 104,\n",
              "   '√É¬´': 105,\n",
              "   'ƒ†them': 106,\n",
              "   'ƒ†years': 107,\n",
              "   \"'\": 108,\n",
              "   'ƒ†do': 109,\n",
              "   'ƒ†your': 110,\n",
              "   'ƒ†-': 111,\n",
              "   'ƒ†1': 112,\n",
              "   '\"': 113,\n",
              "   'ƒ†if': 114,\n",
              "   'ƒ†could': 115,\n",
              "   '?': 116,\n",
              "   'ƒ†no': 117,\n",
              "   'i': 118,\n",
              "   'm': 119,\n",
              "   'ƒ†get': 120,\n",
              "   'ƒ†U': 121,\n",
              "   'ƒ†now': 122,\n",
              "   'ƒ†him': 123,\n",
              "   'ƒ†back': 124,\n",
              "   'ƒ†But': 125,\n",
              "   'ƒ†√¢ƒ¢ƒµ': 126,\n",
              "   'ƒ†my': 127,\n",
              "   \"ƒ†'\": 128,\n",
              "   'ƒ†only': 129,\n",
              "   'ƒ†three': 130,\n",
              "   ';': 131,\n",
              "   'ƒ†2': 132,\n",
              "   'The': 133,\n",
              "   '1': 134,\n",
              "   'ƒ†percent': 135,\n",
              "   'ƒ†against': 136,\n",
              "   'ƒ†before': 137,\n",
              "   'ƒ†company': 138,\n",
              "   'o': 139,\n",
              "   'ƒ†Trump': 140,\n",
              "   'ƒ†how': 141,\n",
              "   'ƒ†because': 142,\n",
              "   'ƒ†any': 143,\n",
              "   'ƒ†most': 144,\n",
              "   'ƒ†being': 145,\n",
              "   'ƒ†make': 146,\n",
              "   'ƒ†where': 147,\n",
              "   'ƒ†during': 148,\n",
              "   'ƒ†through': 149,\n",
              "   'ƒ†while': 150,\n",
              "   '000': 151,\n",
              "   'ƒ†This': 152,\n",
              "   'ƒ†million': 153,\n",
              "   'ing': 154,\n",
              "   'ƒ†3': 155,\n",
              "   'ƒ†made': 156,\n",
              "   'ƒ†well': 157,\n",
              "   'ƒ†10': 158,\n",
              "   'ƒ†down': 159,\n",
              "   'ƒ†off': 160,\n",
              "   'ƒ†says': 161,\n",
              "   'ƒ†me': 162,\n",
              "   'ƒ†B': 163,\n",
              "   'ƒ†going': 164,\n",
              "   'ƒ†team': 165,\n",
              "   'ƒ†We': 166,\n",
              "   'ƒ†those': 167,\n",
              "   'ƒ†government': 168,\n",
              "   'ƒ†way': 169,\n",
              "   'We': 170,\n",
              "   'ƒ†many': 171,\n",
              "   'ƒ†then': 172,\n",
              "   'ƒ†work': 173,\n",
              "   'ƒ†told': 174,\n",
              "   'com': 175,\n",
              "   '2': 176,\n",
              "   'ƒ†game': 177,\n",
              "   'ƒ†And': 178,\n",
              "   'in': 179,\n",
              "   'year': 180,\n",
              "   'ƒ†p': 181,\n",
              "   'ƒ†very': 182,\n",
              "   'ƒ†day': 183,\n",
              "   'ƒ†home': 184,\n",
              "   'ƒ†take': 185,\n",
              "   'ƒ†week': 186,\n",
              "   'ƒ†since': 187,\n",
              "   'ƒ†New': 188,\n",
              "   'ƒ†may': 189,\n",
              "   'ƒ†even': 190,\n",
              "   'ƒ†season': 191,\n",
              "   'ƒ†see': 192,\n",
              "   'ƒ†2017': 193,\n",
              "   'ƒ†state': 194,\n",
              "   'ƒ†5': 195,\n",
              "   'ed': 196,\n",
              "   'ƒ†should': 197,\n",
              "   'ƒ†around': 198,\n",
              "   'ƒ†2018': 199,\n",
              "   'ƒ†second': 200,\n",
              "   'ƒ†us': 201,\n",
              "   'ƒ†still': 202,\n",
              "   'ƒ†much': 203,\n",
              "   'ƒ†4': 204,\n",
              "   'ƒ†good': 205,\n",
              "   'ƒ†think': 206,\n",
              "   '%': 207,\n",
              "   'ƒ†S': 208,\n",
              "   'ƒ†these': 209,\n",
              "   'ƒ†market': 210,\n",
              "   'ƒ†D': 211,\n",
              "   'th': 212,\n",
              "   'ƒ†go': 213,\n",
              "   \"'re\": 214,\n",
              "   'ƒ†such': 215,\n",
              "   'ƒ†know': 216,\n",
              "   'ƒ†including': 217,\n",
              "   'ƒ†don': 218,\n",
              "   'y': 219,\n",
              "   'ƒ†next': 220,\n",
              "   'ƒ†P': 221,\n",
              "   'ƒ†did': 222,\n",
              "   'ƒ†under': 223,\n",
              "   'ƒ†say': 224,\n",
              "   'en': 225,\n",
              "   'ƒ†L': 226,\n",
              "   'ƒ†between': 227,\n",
              "   'ƒ†per': 228,\n",
              "   'ƒ†K': 229,\n",
              "   'ƒ†C': 230,\n",
              "   'ƒ†6': 231,\n",
              "   'ƒ†world': 232,\n",
              "   'ƒ†part': 233,\n",
              "   'ƒ†N': 234,\n",
              "   'ƒ†right': 235,\n",
              "   'ƒ†want': 236,\n",
              "   'ƒ†four': 237,\n",
              "   '),': 238,\n",
              "   'ƒ†high': 239,\n",
              "   'ƒ†need': 240,\n",
              "   're': 241,\n",
              "   'e': 242,\n",
              "   'It': 243,\n",
              "   'ƒ†help': 244,\n",
              "   '5': 245,\n",
              "   '3': 246,\n",
              "   'ƒ†country': 247,\n",
              "   'ƒ†R': 248,\n",
              "   'ƒ†police': 249,\n",
              "   'A': 250,\n",
              "   'ƒ†long': 251,\n",
              "   'ƒ†They': 252,\n",
              "   'ƒ†end': 253,\n",
              "   'er': 254,\n",
              "   'ƒ†T': 255,\n",
              "   'ƒ†M': 256,\n",
              "   'u': 257,\n",
              "   'ƒ†both': 258,\n",
              "   'ƒ†here': 259,\n",
              "   'an': 260,\n",
              "   'on': 261,\n",
              "   'ƒ†7': 262,\n",
              "   'ƒ†de': 263,\n",
              "   'ƒ†She': 264,\n",
              "   'ƒ†business': 265,\n",
              "   'ƒ†report': 266,\n",
              "   'j': 267,\n",
              "   'ers': 268,\n",
              "   'ƒ†really': 269,\n",
              "   'ƒ†President': 270,\n",
              "   'ar': 271,\n",
              "   'ƒ†G': 272,\n",
              "   'ƒ†Friday': 273,\n",
              "   'ƒ†F': 274,\n",
              "   'ƒ†best': 275,\n",
              "   'ƒ†same': 276,\n",
              "   'ƒ†another': 277,\n",
              "   'ƒ†set': 278,\n",
              "   'old': 279,\n",
              "   'ƒ†That': 280,\n",
              "   'as': 281,\n",
              "   'n': 282,\n",
              "   'ƒ†come': 283,\n",
              "   'ƒ†family': 284,\n",
              "   'ƒ†public': 285,\n",
              "   'ƒ†For': 286,\n",
              "   'ƒ†As': 287,\n",
              "   '0': 288,\n",
              "   'ƒ†H': 289,\n",
              "   'ƒ†8': 290,\n",
              "   'ƒ†20': 291,\n",
              "   'ƒ†five': 292,\n",
              "   'es': 293,\n",
              "   'ƒ†Tuesday': 294,\n",
              "   'ƒ†n': 295,\n",
              "   'ƒ†Thursday': 296,\n",
              "   'ƒ†quarter': 297,\n",
              "   'h': 298,\n",
              "   'ƒ†top': 299,\n",
              "   'ƒ†got': 300,\n",
              "   'ƒ†life': 301,\n",
              "   'ƒ†Monday': 302,\n",
              "   'ƒ†found': 303,\n",
              "   'ƒ†use': 304,\n",
              "   'ƒ†W': 305,\n",
              "   '4': 306,\n",
              "   'ƒ†Wednesday': 307,\n",
              "   'ƒ†own': 308,\n",
              "   'ƒ†according': 309,\n",
              "   'ƒ†play': 310,\n",
              "   'ƒ†show': 311,\n",
              "   'ƒ†St': 312,\n",
              "   'ƒ†man': 313,\n",
              "   'ƒ†left': 314,\n",
              "   'ƒ†United': 315,\n",
              "   'ƒ†12': 316,\n",
              "   'ƒ†place': 317,\n",
              "   'ƒ†If': 318,\n",
              "   'ƒ†lot': 319,\n",
              "   'ƒ†former': 320,\n",
              "   'ƒ†0': 321,\n",
              "   ').': 322,\n",
              "   'ƒ†support': 323,\n",
              "   'ie': 324,\n",
              "   'ƒ†billion': 325,\n",
              "   'ƒ†t': 326,\n",
              "   'ƒ†shares': 327,\n",
              "   '!': 328,\n",
              "   'z': 329,\n",
              "   'k': 330,\n",
              "   'ƒ†State': 331,\n",
              "   'ƒ†points': 332,\n",
              "   'ƒ†group': 333,\n",
              "   'ƒ†school': 334,\n",
              "   'ƒ†information': 335,\n",
              "   'ƒ†2016': 336,\n",
              "   'al': 337,\n",
              "   'r': 338,\n",
              "   'ƒ†win': 339,\n",
              "   'ƒ†news': 340,\n",
              "   'ƒ†used': 341,\n",
              "   'ƒ†put': 342,\n",
              "   'ƒ†city': 343,\n",
              "   'ƒ†J': 344,\n",
              "   'ƒ†There': 345,\n",
              "   'ƒ†number': 346,\n",
              "   'C': 347,\n",
              "   \"'ve\": 348,\n",
              "   'ƒ†each': 349,\n",
              "   'ƒ†too': 350,\n",
              "   'ƒ†won': 351,\n",
              "   'ly': 352,\n",
              "   'ƒ†month': 353,\n",
              "   'is': 354,\n",
              "   'ƒ†added': 355,\n",
              "   'ƒ†look': 356,\n",
              "   'ƒ†better': 357,\n",
              "   'ƒ†every': 358,\n",
              "   'ƒ†&': 359,\n",
              "   'ƒ†days': 360,\n",
              "   'ƒ†9': 361,\n",
              "   'ƒ†took': 362,\n",
              "   'ƒ†night': 363,\n",
              "   'ƒ†e': 364,\n",
              "   'ƒ†11': 365,\n",
              "   'os': 366,\n",
              "   'ƒ†few': 367,\n",
              "   'or': 368,\n",
              "   'ƒ†North': 369,\n",
              "   'ƒ†You': 370,\n",
              "   'ƒ†third': 371,\n",
              "   'ƒ†great': 372,\n",
              "   'ƒ†called': 373,\n",
              "   'ƒ†On': 374,\n",
              "   'ƒ†past': 375,\n",
              "   'ƒ†came': 376,\n",
              "   'ƒ†months': 377,\n",
              "   'ƒ†Saturday': 378,\n",
              "   'ƒ†15': 379,\n",
              "   'ƒ†big': 380,\n",
              "   'ƒ†E': 381,\n",
              "   'ƒ†US': 382,\n",
              "   'ƒ†things': 383,\n",
              "   'ƒ†O': 384,\n",
              "   'ƒ†d': 385,\n",
              "   'ƒ†start': 386,\n",
              "   'B': 387,\n",
              "   'ƒ†stock': 388,\n",
              "   'ƒ†30': 389,\n",
              "   'ƒ†women': 390,\n",
              "   'ƒ†South': 391,\n",
              "   'ƒ†May': 392,\n",
              "   'ƒ†never': 393,\n",
              "   'ƒ†president': 394,\n",
              "   'ƒ†Sunday': 395,\n",
              "   'ƒ†without': 396,\n",
              "   'man': 397,\n",
              "   '8': 398,\n",
              "   'ƒ†didn': 399,\n",
              "   'ƒ†local': 400,\n",
              "   '6': 401,\n",
              "   'ƒ†something': 402,\n",
              "   'ƒ†case': 403,\n",
              "   'ƒ†All': 404,\n",
              "   'it': 405,\n",
              "   '7': 406,\n",
              "   'ƒ†So': 407,\n",
              "   'ƒ†children': 408,\n",
              "   'ƒ†away': 409,\n",
              "   'ƒ†little': 410,\n",
              "   'ƒ†six': 411,\n",
              "   'ƒ†City': 412,\n",
              "   'ƒ†County': 413,\n",
              "   'ƒ†data': 414,\n",
              "   'at': 415,\n",
              "   'ƒ†already': 416,\n",
              "   'd': 417,\n",
              "   'ƒ†money': 418,\n",
              "   'ƒ†early': 419,\n",
              "   'ƒ†across': 420,\n",
              "   'ƒ†expected': 421,\n",
              "   'ƒ†run': 422,\n",
              "   'ƒ†later': 423,\n",
              "   'am': 424,\n",
              "   'ƒ†price': 425,\n",
              "   'ƒ†games': 426,\n",
              "   'ƒ†Mr': 427,\n",
              "   'b': 428,\n",
              "   'ƒ†might': 429,\n",
              "   'ƒ†different': 430,\n",
              "   'ƒ†reported': 431,\n",
              "   'ƒ†deal': 432,\n",
              "   'ƒ†media': 433,\n",
              "   'ƒ†growth': 434,\n",
              "   'ƒ†community': 435,\n",
              "   'ƒ†China': 436,\n",
              "   \"'m\": 437,\n",
              "   'c': 438,\n",
              "   'ƒ†went': 439,\n",
              "   'ƒ†No': 440,\n",
              "   'ƒ†able': 441,\n",
              "   'ƒ†making': 442,\n",
              "   'ƒ†area': 443,\n",
              "   'ƒ†far': 444,\n",
              "   'ƒ†statement': 445,\n",
              "   'ƒ†House': 446,\n",
              "   'ƒ†working': 447,\n",
              "   'M': 448,\n",
              "   'ƒ†k': 449,\n",
              "   'ƒ†seen': 450,\n",
              "   'ƒ†companies': 451,\n",
              "   'ƒ†today': 452,\n",
              "   'ƒ†members': 453,\n",
              "   'ƒ†until': 454,\n",
              "   'ƒ†full': 455,\n",
              "   'ƒ†again': 456,\n",
              "   'ƒ†half': 457,\n",
              "   'ƒ†share': 458,\n",
              "   'le': 459,\n",
              "   'ƒ†always': 460,\n",
              "   'ƒ†court': 461,\n",
              "   'l': 462,\n",
              "   'and': 463,\n",
              "   'ƒ†change': 464,\n",
              "   'ƒ†find': 465,\n",
              "   '9': 466,\n",
              "   'ƒ†system': 467,\n",
              "   'ƒ†V': 468,\n",
              "   'ƒ†York': 469,\n",
              "   'ƒ†American': 470,\n",
              "   'ƒ†head': 471,\n",
              "   'ƒ†players': 472,\n",
              "   'ƒ†does': 473,\n",
              "   'ƒ†health': 474,\n",
              "   'ƒ†m': 475,\n",
              "   'ƒ†power': 476,\n",
              "   'ƒ†point': 477,\n",
              "   'ƒ†hit': 478,\n",
              "   'ƒ†.': 479,\n",
              "   'ƒ†--': 480,\n",
              "   'ƒ†free': 481,\n",
              "   '.,': 482,\n",
              "   'ƒ†lead': 483,\n",
              "   'ƒ†several': 484,\n",
              "   'ƒ†recent': 485,\n",
              "   'ƒ†call': 486,\n",
              "   'N': 487,\n",
              "   'ƒ†law': 488,\n",
              "   'ƒ†keep': 489,\n",
              "   'ƒ†open': 490,\n",
              "   'ƒ†News': 491,\n",
              "   'ƒ†give': 492,\n",
              "   'ia': 493,\n",
              "   'ƒ†March': 494,\n",
              "   'D': 495,\n",
              "   'ƒ†National': 496,\n",
              "   'ƒ†At': 497,\n",
              "   'ƒ†times': 498,\n",
              "   'ƒ†future': 499,\n",
              "   'R': 500,\n",
              "   'ƒ†14': 501,\n",
              "   'ƒ†June': 502,\n",
              "   'ƒ†officials': 503,\n",
              "   'ƒ†18': 504,\n",
              "   'ƒ†important': 505,\n",
              "   'f': 506,\n",
              "   'ƒ†final': 507,\n",
              "   'ƒ†13': 508,\n",
              "   'ƒ†One': 509,\n",
              "   'P': 510,\n",
              "   'ƒ†following': 511,\n",
              "   'ƒ†car': 512,\n",
              "   'ƒ†least': 513,\n",
              "   'ƒ†water': 514,\n",
              "   'ƒ†event': 515,\n",
              "   'ƒ†line': 516,\n",
              "   'ƒ†move': 517,\n",
              "   'ƒ†services': 518,\n",
              "   'ƒ†having': 519,\n",
              "   'ƒ†When': 520,\n",
              "   'ƒ†students': 521,\n",
              "   'ƒ†Police': 522,\n",
              "   'el': 523,\n",
              "   'ƒ†am': 524,\n",
              "   'ƒ†Z': 525,\n",
              "   'ƒ†side': 526,\n",
              "   'ƒ†story': 527,\n",
              "   'ƒ†due': 528,\n",
              "   'ƒ†meeting': 529,\n",
              "   'K': 530,\n",
              "   'ƒ†must': 531,\n",
              "   'ƒ†States': 532,\n",
              "   'ƒ†likely': 533,\n",
              "   'G': 534,\n",
              "   'ƒ†continue': 535,\n",
              "   'ƒ†ago': 536,\n",
              "   'ƒ†party': 537,\n",
              "   'ƒ†major': 538,\n",
              "   'ƒ†industry': 539,\n",
              "   'ƒ†less': 540,\n",
              "   '30': 541,\n",
              "   'ƒ†un': 542,\n",
              "   'ƒ†hard': 543,\n",
              "   'ƒ†service': 544,\n",
              "   'ƒ†16': 545,\n",
              "   'ƒ†looking': 546,\n",
              "   'ƒ†held': 547,\n",
              "   've': 548,\n",
              "   'ƒ†whether': 549,\n",
              "   'ƒ†July': 550,\n",
              "   'ƒ†taken': 551,\n",
              "   'ƒ†along': 552,\n",
              "   'ƒ†asked': 553,\n",
              "   'ƒ†started': 554,\n",
              "   'ƒ†become': 555,\n",
              "   'ƒ†forward': 556,\n",
              "   'ƒ†research': 557,\n",
              "   'ƒ†office': 558,\n",
              "   'ƒ†political': 559,\n",
              "   'to': 560,\n",
              "   'ƒ†together': 561,\n",
              "   'ƒ†getting': 562,\n",
              "   'ƒ†plan': 563,\n",
              "   'ƒ†25': 564,\n",
              "   'T': 565,\n",
              "   'ƒ†among': 566,\n",
              "   'ƒ†coming': 567,\n",
              "   'ƒ†decision': 568,\n",
              "   'ƒ†video': 569,\n",
              "   'ƒ†2015': 570,\n",
              "   'g': 571,\n",
              "   'ƒ†After': 572,\n",
              "   'ƒ†security': 573,\n",
              "   'L': 574,\n",
              "   'ƒ†care': 575,\n",
              "   'ƒ†given': 576,\n",
              "   'ƒ†available': 577,\n",
              "   '√¢ƒ¢ƒ∂': 578,\n",
              "   'ƒ†s': 579,\n",
              "   'ƒ†West': 580,\n",
              "   \"'ll\": 581,\n",
              "   'ƒ†pay': 582,\n",
              "   'ƒ†near': 583,\n",
              "   'ƒ†saying': 584,\n",
              "   'ƒ†announced': 585,\n",
              "   'ƒ†program': 586,\n",
              "   'ƒ†April': 587,\n",
              "   'ƒ†real': 588,\n",
              "   'ƒ†University': 589,\n",
              "   'ƒ†With': 590,\n",
              "   'AP': 591,\n",
              "   'ƒ†social': 592,\n",
              "   'ƒ†close': 593,\n",
              "   'et': 594,\n",
              "   'ƒ†current': 595,\n",
              "   'ƒ†why': 596,\n",
              "   'F': 597,\n",
              "   'ƒ†To': 598,\n",
              "   'ƒ†Twitter': 599,\n",
              "   'ƒ†though': 600,\n",
              "   'ƒ†17': 601,\n",
              "   'ƒ†taking': 602,\n",
              "   'ƒ†Inc': 603,\n",
              "   'ƒ†men': 604,\n",
              "   'w': 605,\n",
              "   'ƒ†comes': 606,\n",
              "   'ley': 607,\n",
              "   'ƒ†doing': 608,\n",
              "   'ƒ†process': 609,\n",
              "   'ƒ†John': 610,\n",
              "   'ch': 611,\n",
              "   '00': 612,\n",
              "   'ƒ†financial': 613,\n",
              "   'ƒ†low': 614,\n",
              "   'ƒ†enough': 615,\n",
              "   'ƒ†While': 616,\n",
              "   'ƒ†further': 617,\n",
              "   'ƒ†post': 618,\n",
              "   'ƒ†feel': 619,\n",
              "   'st': 620,\n",
              "   'ƒ†person': 621,\n",
              "   'ƒ†Facebook': 622,\n",
              "   'ƒ†World': 623,\n",
              "   'ƒ†within': 624,\n",
              "   'ad': 625,\n",
              "   'ƒ†done': 626,\n",
              "   'the': 627,\n",
              "   'ƒ†late': 628,\n",
              "   'ƒ†tax': 629,\n",
              "   'ƒ†doesn': 630,\n",
              "   'ƒ†thing': 631,\n",
              "   'ƒ†national': 632,\n",
              "   'ƒ†job': 633,\n",
              "   'ƒ†using': 634,\n",
              "   'ƒ†However': 635,\n",
              "   'ic': 636,\n",
              "   'ƒ†campaign': 637,\n",
              "   'ƒ†record': 638,\n",
              "   'ƒ†behind': 639,\n",
              "   '://': 640,\n",
              "   'ƒ†Department': 641,\n",
              "   'p': 642,\n",
              "   'ƒ†others': 643,\n",
              "   'ƒ†January': 644,\n",
              "   'ƒ†order': 645,\n",
              "   'ƒ†[': 646,\n",
              "   'ƒ†sales': 647,\n",
              "   'ƒ†yet': 648,\n",
              "   '√Ñ': 649,\n",
              "   'ƒ†small': 650,\n",
              "   'ƒ†series': 651,\n",
              "   'ƒ†face': 652,\n",
              "   'ƒ†What': 653,\n",
              "   'ƒ†50': 654,\n",
              "   'ƒ†ever': 655,\n",
              "   'ƒ†earlier': 656,\n",
              "   'ƒ†love': 657,\n",
              "   'up': 658,\n",
              "   'ƒ†rights': 659,\n",
              "   'ƒ†An': 660,\n",
              "   'ist': 661,\n",
              "   'ƒ†morning': 662,\n",
              "   'ƒ†Washington': 663,\n",
              "   'ƒ†young': 664,\n",
              "   'ƒ†latest': 665,\n",
              "   'ƒ†India': 666,\n",
              "   'ƒ†trying': 667,\n",
              "   'ƒ†fire': 668,\n",
              "   'ƒ†led': 669,\n",
              "   'ƒ†strong': 670,\n",
              "   'ƒ†return': 671,\n",
              "   'ƒ†level': 672,\n",
              "   'O': 673,\n",
              "   'ƒ†average': 674,\n",
              "   'ƒ†period': 675,\n",
              "   'ƒ†experience': 676,\n",
              "   'ak': 677,\n",
              "   'ƒ†possible': 678,\n",
              "   'ƒ†believe': 679,\n",
              "   'ƒ†include': 680,\n",
              "   'ƒ†oil': 681,\n",
              "   'ƒ†recently': 682,\n",
              "   'ƒ†once': 683,\n",
              "   'ƒ†known': 684,\n",
              "   'ƒ†lost': 685,\n",
              "   'ƒ†sure': 686,\n",
              "   'us': 687,\n",
              "   'ƒ†weeks': 688,\n",
              "   'ƒ†food': 689,\n",
              "   'ƒ†reports': 690,\n",
              "   'ƒ†rating': 691,\n",
              "   'ƒ†Minister': 692,\n",
              "   'ƒ†woman': 693,\n",
              "   'ƒ†provide': 694,\n",
              "   'ƒ†project': 695,\n",
              "   'ƒ†issue': 696,\n",
              "   'ƒ†live': 697,\n",
              "   '10': 698,\n",
              "   'ƒ†clear': 699,\n",
              "   'he': 700,\n",
              "   'ƒ†cost': 701,\n",
              "   'ƒ†played': 702,\n",
              "   'ƒ†released': 703,\n",
              "   'ƒ†coach': 704,\n",
              "   'v': 705,\n",
              "   'ƒ†24': 706,\n",
              "   'ƒ†seven': 707,\n",
              "   'ƒ†plans': 708,\n",
              "   'ƒ†development': 709,\n",
              "   'ur': 710,\n",
              "   'ƒ∫': 711,\n",
              "   'ƒ†increase': 712,\n",
              "   'This': 713,\n",
              "   'ƒ†policy': 714,\n",
              "   'ƒ†cent': 715,\n",
              "   'ƒ†based': 716,\n",
              "   'E': 717,\n",
              "   'il': 718,\n",
              "   'ƒ†December': 719,\n",
              "   'ƒ†global': 720,\n",
              "   'ƒ†trade': 721,\n",
              "   'ƒ†hours': 722,\n",
              "   'ƒ†higher': 723,\n",
              "   'ƒ†goal': 724,\n",
              "   'H': 725,\n",
              "   'ƒ†Al': 726,\n",
              "   'ƒ†100': 727,\n",
              "   'ƒ†minutes': 728,\n",
              "   'ƒ†election': 729,\n",
              "   'ƒ†America': 730,\n",
              "   'ƒ†rate': 731,\n",
              "   'ƒ†Ch': 732,\n",
              "   'ƒ†21': 733,\n",
              "   '...': 734,\n",
              "   'ƒ†White': 735,\n",
              "   'ƒ†director': 736,\n",
              "   'ƒ†position': 737,\n",
              "   'ƒ†shot': 738,\n",
              "   'ƒ†large': 739,\n",
              "   'ƒ†c': 740,\n",
              "   'ƒ†b': 741,\n",
              "   ']': 742,\n",
              "   'ƒ†issues': 743,\n",
              "   'ƒ†death': 744,\n",
              "   'ƒ†building': 745,\n",
              "   'ƒ†total': 746,\n",
              "   'ƒ†often': 747,\n",
              "   'ƒ†v': 748,\n",
              "   'ƒ†countries': 749,\n",
              "   'ƒ†history': 750,\n",
              "   'ƒ†outside': 751,\n",
              "   'ƒ†federal': 752,\n",
              "   'ƒ†19': 753,\n",
              "   'ƒ†fact': 754,\n",
              "   'ƒ†High': 755,\n",
              "   'ƒ†career': 756,\n",
              "   'im': 757,\n",
              "   'ƒ†international': 758,\n",
              "   'ƒ†November': 759,\n",
              "   'ƒ†front': 760,\n",
              "   'ƒ†kind': 761,\n",
              "   'ƒ†key': 762,\n",
              "   'ra': 763,\n",
              "   'ƒ†San': 764,\n",
              "   'ƒ†short': 765,\n",
              "   'ƒ†name': 766,\n",
              "   'ƒ†According': 767,\n",
              "   'ƒ†course': 768,\n",
              "   'ƒ†re': 769,\n",
              "   'ƒ†wanted': 770,\n",
              "   'W': 771,\n",
              "   'ƒ†September': 772,\n",
              "   'ƒ†interest': 773,\n",
              "   'ƒ†role': 774,\n",
              "   'ƒ†results': 775,\n",
              "   'ƒ†economic': 776,\n",
              "   'ƒ†2014': 777,\n",
              "   'ƒ†chance': 778,\n",
              "   'ƒ†October': 779,\n",
              "   'ƒ†special': 780,\n",
              "   'ƒ†official': 781,\n",
              "   'ƒ†needs': 782,\n",
              "   'um': 783,\n",
              "   'ƒ†l': 784,\n",
              "   'ƒ†products': 785,\n",
              "   'ƒ†non': 786,\n",
              "   'ƒ†@': 787,\n",
              "   'ƒ†Bank': 788,\n",
              "   'ƒ†ahead': 789,\n",
              "   'ƒ†house': 790,\n",
              "   'U': 791,\n",
              "   'ƒ†board': 792,\n",
              "   'ƒ†old': 793,\n",
              "   'ƒ†saw': 794,\n",
              "   'ƒ†lower': 795,\n",
              "   'ƒ†European': 796,\n",
              "   'ƒ†control': 797,\n",
              "   'ƒ†Russia': 798,\n",
              "   'ƒ†eight': 799,\n",
              "   'ƒ†release': 800,\n",
              "   'ƒ†potential': 801,\n",
              "   'ƒ†thought': 802,\n",
              "   'ƒ†investigation': 803,\n",
              "   'ƒ†online': 804,\n",
              "   'based': 805,\n",
              "   'ƒ†technology': 806,\n",
              "   'ƒ†Donald': 807,\n",
              "   'id': 808,\n",
              "   'ƒ†body': 809,\n",
              "   'ƒ†risk': 810,\n",
              "   'ian': 811,\n",
              "   'ƒ†capital': 812,\n",
              "   'ƒ†staff': 813,\n",
              "   'ƒ†action': 814,\n",
              "   'ƒ†League': 815,\n",
              "   'ƒ†playing': 816,\n",
              "   'ƒ†makes': 817,\n",
              "   'ƒ†almost': 818,\n",
              "   'ƒ†performance': 819,\n",
              "   'ƒ†22': 820,\n",
              "   'ƒ†g': 821,\n",
              "   'ƒ†film': 822,\n",
              "   'ƒ†nearly': 823,\n",
              "   'ƒ†Center': 824,\n",
              "   'ƒ†visit': 825,\n",
              "   'ƒ†Group': 826,\n",
              "   'ƒ†bank': 827,\n",
              "   'ƒ†bit': 828,\n",
              "   'ƒ†received': 829,\n",
              "   'ƒ†August': 830,\n",
              "   'ƒ†military': 831,\n",
              "   'ƒ†His': 832,\n",
              "   'ine': 833,\n",
              "   'ƒ†chief': 834,\n",
              "   'ƒ†School': 835,\n",
              "   'ƒ†bring': 836,\n",
              "   'ƒ†Court': 837,\n",
              "   'ƒ†(@': 838,\n",
              "   'ƒ†means': 839,\n",
              "   'ƒ†Sh': 840,\n",
              "   'ƒ†fans': 841,\n",
              "   'ƒ†se': 842,\n",
              "   'ƒ†40': 843,\n",
              "   '20': 844,\n",
              "   '\".': 845,\n",
              "   'V': 846,\n",
              "   'ƒ†cut': 847,\n",
              "   'ƒ†killed': 848,\n",
              "   'ƒ†#': 849,\n",
              "   'ƒ†prices': 850,\n",
              "   'ƒ†gave': 851,\n",
              "   'ƒ†Street': 852,\n",
              "   'ir': 853,\n",
              "   'ƒ†Y': 854,\n",
              "   'ƒ†currently': 855,\n",
              "   'ƒ†f': 856,\n",
              "   'ay': 857,\n",
              "   'ne': 858,\n",
              "   'te': 859,\n",
              "   'ƒ†try': 860,\n",
              "   'ƒ†Park': 861,\n",
              "   'ƒ•': 862,\n",
              "   'J': 863,\n",
              "   'ƒ†question': 864,\n",
              "   'ƒ†hand': 865,\n",
              "   'ƒ†economy': 866,\n",
              "   'ƒ†investors': 867,\n",
              "   'able': 868,\n",
              "   'ƒ†player': 869,\n",
              "   'ƒ†By': 870,\n",
              "   'ƒ†David': 871,\n",
              "   'ƒ†loss': 872,\n",
              "   'ab': 873,\n",
              "   'ƒ†below': 874,\n",
              "   'ƒ†wrote': 875,\n",
              "   'co': 876,\n",
              "   'ate': 877,\n",
              "   'ƒ†running': 878,\n",
              "   'un': 879,\n",
              "   'ƒ†began': 880,\n",
              "   'ƒ†single': 881,\n",
              "   'ƒ†field': 882,\n",
              "   'ƒ†23': 883,\n",
              "   'ƒ†leader': 884,\n",
              "   'ƒ†w': 885,\n",
              "   'ƒ†California': 886,\n",
              "   'ƒ†fourth': 887,\n",
              "   'ƒ†actually': 888,\n",
              "   'ƒ†list': 889,\n",
              "   'll': 890,\n",
              "   'ƒ†couple': 891,\n",
              "   'ƒ†study': 892,\n",
              "   'ƒ†teams': 893,\n",
              "   'He': 894,\n",
              "   'ah': 895,\n",
              "   'ƒ†Canada': 896,\n",
              "   'ƒ†la': 897,\n",
              "   'ƒ†result': 898,\n",
              "   'ƒ†access': 899,\n",
              "   'ƒ†vote': 900,\n",
              "   'ƒ†More': 901,\n",
              "   'ƒ†February': 902,\n",
              "   'ƒ†revenue': 903,\n",
              "   'ƒ†offer': 904,\n",
              "   'ƒ†let': 905,\n",
              "   'ier': 906,\n",
              "   'ƒ†buy': 907,\n",
              "   'ƒ†attack': 908,\n",
              "   'ƒ†black': 909,\n",
              "   'ƒ†r': 910,\n",
              "   'ƒ†areas': 911,\n",
              "   'ƒ†stop': 912,\n",
              "   'ƒ†impact': 913,\n",
              "   'ƒ†match': 914,\n",
              "   'ƒ†investment': 915,\n",
              "   'ƒ†customers': 916,\n",
              "   'ƒ†leaders': 917,\n",
              "   'ies': 918,\n",
              "   'ƒ†member': 919,\n",
              "   'ƒ†child': 920,\n",
              "   'ƒ†road': 921,\n",
              "   'ul': 922,\n",
              "   'ƒ†value': 923,\n",
              "   'ƒ†shows': 924,\n",
              "   'ƒ†Dr': 925,\n",
              "   'ƒ†De': 926,\n",
              "   'ant': 927,\n",
              "   'ƒ†London': 928,\n",
              "   'ƒ†room': 929,\n",
              "   'ƒ†music': 930,\n",
              "   'ƒ†production': 931,\n",
              "   'ƒ†anything': 932,\n",
              "   'ƒ†firm': 933,\n",
              "   'ƒ†biggest': 934,\n",
              "   'ƒ†air': 935,\n",
              "   'ƒ†problem': 936,\n",
              "   'ƒ†general': 937,\n",
              "   'ƒ†wasn': 938,\n",
              "   'ƒ†i': 939,\n",
              "   'ƒ†private': 940,\n",
              "   'ƒ†especially': 941,\n",
              "   'ƒ†administration': 942,\n",
              "   'ƒ†additional': 943,\n",
              "   'ƒ†Co': 944,\n",
              "   'ƒ†opportunity': 945,\n",
              "   'ƒ†hold': 946,\n",
              "   '&': 947,\n",
              "   'ƒ†matter': 948,\n",
              "   'ƒ†senior': 949,\n",
              "   'ƒ†club': 950,\n",
              "   'ƒ†someone': 951,\n",
              "   'ƒ†√É': 952,\n",
              "   'ƒ†East': 953,\n",
              "   'ƒ†2019': 954,\n",
              "   \".'\": 955,\n",
              "   'ƒ†needed': 956,\n",
              "   'ƒ†James': 957,\n",
              "   'time': 958,\n",
              "   'ƒ†however': 959,\n",
              "   'ƒ†everything': 960,\n",
              "   'ƒ†everyone': 961,\n",
              "   'ƒ†died': 962,\n",
              "   'ƒ†involved': 963,\n",
              "   'ƒ†friends': 964,\n",
              "   'ƒ†isn': 965,\n",
              "   'ƒ†worth': 966,\n",
              "   'ik': 967,\n",
              "   'ƒ†Cup': 968,\n",
              "   'ƒ†showed': 969,\n",
              "   'There': 970,\n",
              "   'ƒ†28': 971,\n",
              "   'ƒ†meet': 972,\n",
              "   'ƒ†26': 973,\n",
              "   'ƒ†27': 974,\n",
              "   'Y': 975,\n",
              "   'ƒ†region': 976,\n",
              "   'ƒ†Press': 977,\n",
              "   'ƒ†Now': 978,\n",
              "   'ƒ†son': 979,\n",
              "   'ƒ†space': 980,\n",
              "   'ƒ†leading': 981,\n",
              "   'ƒ†states': 982,\n",
              "   'ƒ†weekend': 983,\n",
              "   'ƒ†√Ç¬£': 984,\n",
              "   'ƒ†mother': 985,\n",
              "   'ƒ†previous': 986,\n",
              "   'ƒ†UK': 987,\n",
              "   'ƒ†Michael': 988,\n",
              "   'ƒ†leave': 989,\n",
              "   'est': 990,\n",
              "   'em': 991,\n",
              "   'ƒ†z': 992,\n",
              "   'ƒ†Some': 993,\n",
              "   'ors': 994,\n",
              "   'out': 995,\n",
              "   '15': 996,\n",
              "   'ƒ†war': 997,\n",
              "   'ƒ†website': 998,\n",
              "   'ƒ†star': 999,\n",
              "   ...},\n",
              "  'merges': ['ƒ† t',\n",
              "   'ƒ† a',\n",
              "   'h e',\n",
              "   'i n',\n",
              "   'r e',\n",
              "   'o n',\n",
              "   'ƒ†t he',\n",
              "   'e r',\n",
              "   'ƒ† s',\n",
              "   'a t',\n",
              "   'ƒ† w',\n",
              "   'ƒ† o',\n",
              "   'e n',\n",
              "   'ƒ† c',\n",
              "   'i t',\n",
              "   'i s',\n",
              "   'a n',\n",
              "   'o r',\n",
              "   'e s',\n",
              "   'ƒ† b',\n",
              "   'e d',\n",
              "   'ƒ† f',\n",
              "   'in g',\n",
              "   'ƒ† p',\n",
              "   'o u',\n",
              "   'ƒ†a n',\n",
              "   'a l',\n",
              "   'a r',\n",
              "   'ƒ†t o',\n",
              "   'ƒ† m',\n",
              "   'ƒ†o f',\n",
              "   'ƒ† in',\n",
              "   'ƒ† d',\n",
              "   'ƒ† h',\n",
              "   'ƒ†an d',\n",
              "   'i c',\n",
              "   'a s',\n",
              "   'l e',\n",
              "   'ƒ†t h',\n",
              "   'i on',\n",
              "   'o m',\n",
              "   'l l',\n",
              "   'en t',\n",
              "   'ƒ† n',\n",
              "   'ƒ† l',\n",
              "   's t',\n",
              "   'ƒ† re',\n",
              "   'v e',\n",
              "   'ƒ† e',\n",
              "   'r o',\n",
              "   'l y',\n",
              "   'ƒ†b e',\n",
              "   'ƒ† g',\n",
              "   'ƒ† T',\n",
              "   'c t',\n",
              "   'ƒ† S',\n",
              "   'i d',\n",
              "   'o t',\n",
              "   'ƒ† I',\n",
              "   'u t',\n",
              "   'e t',\n",
              "   'ƒ† A',\n",
              "   'ƒ† is',\n",
              "   'ƒ† on',\n",
              "   'i m',\n",
              "   'a m',\n",
              "   'o w',\n",
              "   'a y',\n",
              "   'a d',\n",
              "   's e',\n",
              "   'ƒ†th at',\n",
              "   'ƒ† C',\n",
              "   'i g',\n",
              "   'ƒ†f or',\n",
              "   'a c',\n",
              "   'ƒ† y',\n",
              "   'v er',\n",
              "   'u r',\n",
              "   'ƒ† u',\n",
              "   'l d',\n",
              "   'ƒ†s t',\n",
              "   'ƒ† M',\n",
              "   \"' s\",\n",
              "   'ƒ† he',\n",
              "   'ƒ† it',\n",
              "   'at ion',\n",
              "   'it h',\n",
              "   'i r',\n",
              "   'c e',\n",
              "   'ƒ†y ou',\n",
              "   'i l',\n",
              "   'ƒ† B',\n",
              "   'ƒ†w h',\n",
              "   'o l',\n",
              "   'ƒ† P',\n",
              "   'ƒ†w ith',\n",
              "   'ƒ† 1',\n",
              "   't er',\n",
              "   'c h',\n",
              "   'ƒ†a s',\n",
              "   'ƒ†w e',\n",
              "   'ƒ† (',\n",
              "   'n d',\n",
              "   'i ll',\n",
              "   'ƒ† D',\n",
              "   'i f',\n",
              "   'ƒ† 2',\n",
              "   'a g',\n",
              "   'er s',\n",
              "   'k e',\n",
              "   'ƒ† \"',\n",
              "   'ƒ† H',\n",
              "   'e m',\n",
              "   'ƒ†c on',\n",
              "   'ƒ† W',\n",
              "   'ƒ† R',\n",
              "   'he r',\n",
              "   'ƒ†w as',\n",
              "   'ƒ† r',\n",
              "   'o d',\n",
              "   'ƒ† F',\n",
              "   'u l',\n",
              "   'at e',\n",
              "   'ƒ†a t',\n",
              "   'r i',\n",
              "   'p p',\n",
              "   'o re',\n",
              "   'ƒ†T he',\n",
              "   'ƒ†s e',\n",
              "   'u s',\n",
              "   'ƒ†p ro',\n",
              "   'ƒ†h a',\n",
              "   'u m',\n",
              "   'ƒ†a re',\n",
              "   'ƒ†d e',\n",
              "   'a in',\n",
              "   'an d',\n",
              "   'ƒ†o r',\n",
              "   'ig h',\n",
              "   'es t',\n",
              "   'is t',\n",
              "   'a b',\n",
              "   'r om',\n",
              "   'ƒ† N',\n",
              "   't h',\n",
              "   'ƒ†c om',\n",
              "   'ƒ† G',\n",
              "   'u n',\n",
              "   'o p',\n",
              "   '0 0',\n",
              "   'ƒ† L',\n",
              "   'ƒ†n ot',\n",
              "   'es s',\n",
              "   'ƒ†e x',\n",
              "   'ƒ† v',\n",
              "   're s',\n",
              "   'ƒ† E',\n",
              "   'e w',\n",
              "   'it y',\n",
              "   'an t',\n",
              "   'ƒ†b y',\n",
              "   'e l',\n",
              "   'o s',\n",
              "   'or t',\n",
              "   'o c',\n",
              "   'q u',\n",
              "   'ƒ†f rom',\n",
              "   'ƒ†ha ve',\n",
              "   'ƒ†s u',\n",
              "   'i ve',\n",
              "   'ou ld',\n",
              "   'ƒ†s h',\n",
              "   'ƒ†th is',\n",
              "   'n t',\n",
              "   'r a',\n",
              "   'p e',\n",
              "   'igh t',\n",
              "   'ar t',\n",
              "   'm ent',\n",
              "   'ƒ†a l',\n",
              "   'u st',\n",
              "   'en d',\n",
              "   '- -',\n",
              "   'al l',\n",
              "   'ƒ† O',\n",
              "   'ac k',\n",
              "   'ƒ†c h',\n",
              "   'ƒ† le',\n",
              "   'i es',\n",
              "   're d',\n",
              "   'ar d',\n",
              "   '√¢ ƒ¢',\n",
              "   'ou t',\n",
              "   'ƒ† J',\n",
              "   'ƒ†a b',\n",
              "   'e ar',\n",
              "   'i v',\n",
              "   'al ly',\n",
              "   'ou r',\n",
              "   'o st',\n",
              "   'g h',\n",
              "   'p t',\n",
              "   'ƒ†p l',\n",
              "   'as t',\n",
              "   'ƒ†c an',\n",
              "   'a k',\n",
              "   'om e',\n",
              "   'u d',\n",
              "   'T he',\n",
              "   'ƒ†h is',\n",
              "   'ƒ†d o',\n",
              "   'ƒ†g o',\n",
              "   'ƒ†h as',\n",
              "   'g e',\n",
              "   \"' t\",\n",
              "   'ƒ† U',\n",
              "   'r ou',\n",
              "   'ƒ†s a',\n",
              "   'ƒ† j',\n",
              "   'ƒ†b ut',\n",
              "   'ƒ†w or',\n",
              "   'ƒ†a ll',\n",
              "   'e ct',\n",
              "   'ƒ† k',\n",
              "   'am e',\n",
              "   'ƒ†w ill',\n",
              "   'o k',\n",
              "   'ƒ†w he',\n",
              "   'ƒ†the y',\n",
              "   'id e',\n",
              "   '0 1',\n",
              "   'f f',\n",
              "   'ic h',\n",
              "   'p l',\n",
              "   't her',\n",
              "   'ƒ†t r',\n",
              "   '. .',\n",
              "   'ƒ†in t',\n",
              "   'i e',\n",
              "   'u re',\n",
              "   'ag e',\n",
              "   'ƒ†n e',\n",
              "   'i al',\n",
              "   'a p',\n",
              "   'in e',\n",
              "   'ic e',\n",
              "   'ƒ†m e',\n",
              "   'ƒ†o ut',\n",
              "   'an s',\n",
              "   'on e',\n",
              "   'on g',\n",
              "   'ion s',\n",
              "   'ƒ†wh o',\n",
              "   'ƒ† K',\n",
              "   'ƒ†u p',\n",
              "   'ƒ†the ir',\n",
              "   'ƒ†a d',\n",
              "   'ƒ† 3',\n",
              "   'ƒ†u s',\n",
              "   'at ed',\n",
              "   'ou s',\n",
              "   'ƒ†m ore',\n",
              "   'u e',\n",
              "   'o g',\n",
              "   'ƒ†S t',\n",
              "   'in d',\n",
              "   'i ke',\n",
              "   'ƒ†s o',\n",
              "   'im e',\n",
              "   'p er',\n",
              "   '. \"',\n",
              "   'b er',\n",
              "   'i z',\n",
              "   'a ct',\n",
              "   'ƒ†on e',\n",
              "   'ƒ†sa id',\n",
              "   'ƒ† -',\n",
              "   'a re',\n",
              "   'ƒ†you r',\n",
              "   'c c',\n",
              "   'ƒ†T h',\n",
              "   'ƒ†c l',\n",
              "   'e p',\n",
              "   'a ke',\n",
              "   'ab le',\n",
              "   'i p',\n",
              "   'ƒ†con t',\n",
              "   'ƒ†wh ich',\n",
              "   'i a',\n",
              "   'ƒ† im',\n",
              "   'ƒ†ab out',\n",
              "   'ƒ†we re',\n",
              "   'ver y',\n",
              "   'u b',\n",
              "   'ƒ†h ad',\n",
              "   'ƒ† en',\n",
              "   'ƒ†com p',\n",
              "   ', \"',\n",
              "   'ƒ†I n',\n",
              "   'ƒ†u n',\n",
              "   'ƒ†a g',\n",
              "   'i re',\n",
              "   'ac e',\n",
              "   'a u',\n",
              "   'ar y',\n",
              "   'ƒ†w ould',\n",
              "   'as s',\n",
              "   'r y',\n",
              "   'ƒ† √¢ƒ¢',\n",
              "   'c l',\n",
              "   'o ok',\n",
              "   'e re',\n",
              "   's o',\n",
              "   'ƒ† V',\n",
              "   'ig n',\n",
              "   'i b',\n",
              "   'ƒ†of f',\n",
              "   'ƒ†t e',\n",
              "   'v en',\n",
              "   'ƒ† Y',\n",
              "   'i le',\n",
              "   'o se',\n",
              "   'it e',\n",
              "   'or m',\n",
              "   'ƒ†2 01',\n",
              "   'ƒ†re s',\n",
              "   'ƒ†m an',\n",
              "   'ƒ†p er',\n",
              "   'ƒ†o ther',\n",
              "   'or d',\n",
              "   'ul t',\n",
              "   'ƒ†be en',\n",
              "   'ƒ†l ike',\n",
              "   'as e',\n",
              "   'an ce',\n",
              "   'k s',\n",
              "   'ay s',\n",
              "   'ow n',\n",
              "   'en ce',\n",
              "   'ƒ†d is',\n",
              "   'ct ion',\n",
              "   'ƒ†an y',\n",
              "   'ƒ†a pp',\n",
              "   'ƒ†s p',\n",
              "   'in t',\n",
              "   'res s',\n",
              "   'ation s',\n",
              "   'a il',\n",
              "   'ƒ† 4',\n",
              "   'ic al',\n",
              "   'ƒ†the m',\n",
              "   'ƒ†he r',\n",
              "   'ou nt',\n",
              "   'ƒ†C h',\n",
              "   'ƒ†a r',\n",
              "   'ƒ† if',\n",
              "   'ƒ†the re',\n",
              "   'ƒ†p e',\n",
              "   'ƒ†y ear',\n",
              "   'a v',\n",
              "   'ƒ†m y',\n",
              "   'ƒ†s ome',\n",
              "   'ƒ†whe n',\n",
              "   'ou gh',\n",
              "   'ac h',\n",
              "   'ƒ†th an',\n",
              "   'r u',\n",
              "   'on d',\n",
              "   'ic k',\n",
              "   'ƒ†o ver',\n",
              "   've l',\n",
              "   'ƒ† qu',\n",
              "   'ƒä ƒä',\n",
              "   'ƒ†s c',\n",
              "   're at',\n",
              "   're e',\n",
              "   'ƒ†I t',\n",
              "   'ou nd',\n",
              "   'p ort',\n",
              "   'ƒ†al so',\n",
              "   'ƒ†p art',\n",
              "   'f ter',\n",
              "   'ƒ†k n',\n",
              "   'ƒ†be c',\n",
              "   'ƒ†t ime',\n",
              "   'en s',\n",
              "   'ƒ† 5',\n",
              "   'op le',\n",
              "   'ƒ†wh at',\n",
              "   'ƒ†n o',\n",
              "   'd u',\n",
              "   'm er',\n",
              "   'an g',\n",
              "   'ƒ†n ew',\n",
              "   '-- --',\n",
              "   'ƒ†g et',\n",
              "   'or y',\n",
              "   'it ion',\n",
              "   'ing s',\n",
              "   'ƒ†j ust',\n",
              "   'ƒ†int o',\n",
              "   'ƒ† 0',\n",
              "   'ent s',\n",
              "   'o ve',\n",
              "   't e',\n",
              "   'ƒ†pe ople',\n",
              "   'ƒ†p re',\n",
              "   'ƒ†it s',\n",
              "   'ƒ†re c',\n",
              "   'ƒ†t w',\n",
              "   'i an',\n",
              "   'ir st',\n",
              "   'ar k',\n",
              "   'or s',\n",
              "   'ƒ†wor k',\n",
              "   'ad e',\n",
              "   'o b',\n",
              "   'ƒ†s he',\n",
              "   'ƒ†o ur',\n",
              "   'w n',\n",
              "   'in k',\n",
              "   'l ic',\n",
              "   'ƒ†1 9',\n",
              "   'ƒ†H e',\n",
              "   'is h',\n",
              "   'nd er',\n",
              "   'au se',\n",
              "   'ƒ†h im',\n",
              "   'on s',\n",
              "   'ƒ† [',\n",
              "   'ƒ† ro',\n",
              "   'f orm',\n",
              "   'i ld',\n",
              "   'at es',\n",
              "   'ver s',\n",
              "   'ƒ†on ly',\n",
              "   'o ll',\n",
              "   'ƒ†s pe',\n",
              "   'c k',\n",
              "   'e ll',\n",
              "   'am p',\n",
              "   'ƒ†a cc',\n",
              "   'ƒ†b l',\n",
              "   'i ous',\n",
              "   'ur n',\n",
              "   'f t',\n",
              "   'o od',\n",
              "   'ƒ†h ow',\n",
              "   'he d',\n",
              "   \"ƒ† '\",\n",
              "   'ƒ†a fter',\n",
              "   'a w',\n",
              "   'ƒ†at t',\n",
              "   'o v',\n",
              "   'n e',\n",
              "   'ƒ†pl ay',\n",
              "   'er v',\n",
              "   'ic t',\n",
              "   'ƒ†c ould',\n",
              "   'it t',\n",
              "   'ƒ†a m',\n",
              "   'ƒ†f irst',\n",
              "   'ƒ† 6',\n",
              "   'ƒ†a ct',\n",
              "   'ƒ† $',\n",
              "   'e c',\n",
              "   'h ing',\n",
              "   'u al',\n",
              "   'u ll',\n",
              "   'ƒ†com m',\n",
              "   'o y',\n",
              "   'o ld',\n",
              "   'c es',\n",
              "   'at er',\n",
              "   'ƒ†f e',\n",
              "   'ƒ†be t',\n",
              "   'w e',\n",
              "   'if f',\n",
              "   'ƒ†tw o',\n",
              "   'oc k',\n",
              "   'ƒ†b ack',\n",
              "   ') .',\n",
              "   'id ent',\n",
              "   'ƒ†u nder',\n",
              "   'rou gh',\n",
              "   'se l',\n",
              "   'x t',\n",
              "   'ƒ†m ay',\n",
              "   'rou nd',\n",
              "   'ƒ†p o',\n",
              "   'p h',\n",
              "   'is s',\n",
              "   'ƒ†d es',\n",
              "   'ƒ†m ost',\n",
              "   'ƒ†d id',\n",
              "   'ƒ†ad d',\n",
              "   'j ect',\n",
              "   'ƒ†in c',\n",
              "   'f ore',\n",
              "   'ƒ†p ol',\n",
              "   'on t',\n",
              "   'ƒ†ag ain',\n",
              "   'cl ud',\n",
              "   'ter n',\n",
              "   'ƒ†kn ow',\n",
              "   'ƒ†ne ed',\n",
              "   'ƒ†con s',\n",
              "   'ƒ†c o',\n",
              "   'ƒ† .',\n",
              "   'ƒ†w ant',\n",
              "   'ƒ†se e',\n",
              "   'ƒ† 7',\n",
              "   'n ing',\n",
              "   'i ew',\n",
              "   'ƒ†Th is',\n",
              "   'c ed',\n",
              "   'ƒ†e ven',\n",
              "   'ƒ†in d',\n",
              "   't y',\n",
              "   'ƒ†W e',\n",
              "   'at h',\n",
              "   'ƒ†the se',\n",
              "   'ƒ†p r',\n",
              "   'ƒ†u se',\n",
              "   'ƒ†bec ause',\n",
              "   'ƒ†f l',\n",
              "   'n g',\n",
              "   'ƒ†n ow',\n",
              "   'ƒ†√¢ƒ¢ ƒµ',\n",
              "   'c om',\n",
              "   'is e',\n",
              "   'ƒ†m ake',\n",
              "   'ƒ†the n',\n",
              "   'ow er',\n",
              "   'ƒ†e very',\n",
              "   'ƒ†U n',\n",
              "   'ƒ†se c',\n",
              "   'os s',\n",
              "   'u ch',\n",
              "   'ƒ†e m',\n",
              "   'ƒ† =',\n",
              "   'ƒ†R e',\n",
              "   'i ed',\n",
              "   'r it',\n",
              "   'ƒ†in v',\n",
              "   'le ct',\n",
              "   'ƒ†su pp',\n",
              "   'at ing',\n",
              "   'ƒ†l ook',\n",
              "   'm an',\n",
              "   'pe ct',\n",
              "   'ƒ† 8',\n",
              "   'ro w',\n",
              "   'ƒ†b u',\n",
              "   'ƒ†whe re',\n",
              "   'if ic',\n",
              "   'ƒ†year s',\n",
              "   'i ly',\n",
              "   'ƒ†d iff',\n",
              "   'ƒ†sh ould',\n",
              "   'ƒ†re m',\n",
              "   'T h',\n",
              "   'I n',\n",
              "   'ƒ†e v',\n",
              "   'd ay',\n",
              "   \"' re\",\n",
              "   'ri b',\n",
              "   'ƒ†re l',\n",
              "   's s',\n",
              "   'ƒ†de f',\n",
              "   'ƒ†r ight',\n",
              "   'ƒ†s y',\n",
              "   ') ,',\n",
              "   'l es',\n",
              "   '00 0',\n",
              "   'he n',\n",
              "   'ƒ†th rough',\n",
              "   'ƒ†T r',\n",
              "   '_ _',\n",
              "   'ƒ†w ay',\n",
              "   'ƒ†d on',\n",
              "   'ƒ† ,',\n",
              "   'ƒ†1 0',\n",
              "   'as ed',\n",
              "   'ƒ†as s',\n",
              "   'ub lic',\n",
              "   'ƒ†re g',\n",
              "   'ƒ†A nd',\n",
              "   'i x',\n",
              "   'ƒ† very',\n",
              "   'ƒ†in clud',\n",
              "   'ot her',\n",
              "   'ƒ†im p',\n",
              "   'ot h',\n",
              "   'ƒ†su b',\n",
              "   'ƒ†√¢ƒ¢ ƒ∂',\n",
              "   'ƒ†be ing',\n",
              "   'ar g',\n",
              "   'ƒ†W h',\n",
              "   '= =',\n",
              "   'ib le',\n",
              "   'ƒ†do es',\n",
              "   'an ge',\n",
              "   'r am',\n",
              "   'ƒ† 9',\n",
              "   'er t',\n",
              "   'p s',\n",
              "   'it ed',\n",
              "   'ation al',\n",
              "   'ƒ†b r',\n",
              "   'ƒ†d own',\n",
              "   'ƒ†man y',\n",
              "   'ak ing',\n",
              "   'ƒ†c all',\n",
              "   'ur ing',\n",
              "   'it ies',\n",
              "   'ƒ†p h',\n",
              "   'ic s',\n",
              "   'al s',\n",
              "   'ƒ†de c',\n",
              "   'at ive',\n",
              "   'en er',\n",
              "   'ƒ†be fore',\n",
              "   'il ity',\n",
              "   'ƒ†we ll',\n",
              "   'ƒ†m uch',\n",
              "   'ers on',\n",
              "   'ƒ†th ose',\n",
              "   'ƒ†su ch',\n",
              "   'ƒ† ke',\n",
              "   'ƒ† end',\n",
              "   'ƒ†B ut',\n",
              "   'as on',\n",
              "   't ing',\n",
              "   'ƒ†l ong',\n",
              "   'e f',\n",
              "   'ƒ†th ink',\n",
              "   'y s',\n",
              "   'ƒ†be l',\n",
              "   'ƒ†s m',\n",
              "   'it s',\n",
              "   'a x',\n",
              "   'ƒ†o wn',\n",
              "   'ƒ†pro v',\n",
              "   'ƒ†s et',\n",
              "   'if e',\n",
              "   'ment s',\n",
              "   'b le',\n",
              "   'w ard',\n",
              "   'ƒ†sh ow',\n",
              "   'ƒ†p res',\n",
              "   'm s',\n",
              "   'om et',\n",
              "   'ƒ†o b',\n",
              "   'ƒ†s ay',\n",
              "   'ƒ†S h',\n",
              "   't s',\n",
              "   'f ul',\n",
              "   'ƒ†e ff',\n",
              "   'ƒ†g u',\n",
              "   'ƒ†in st',\n",
              "   'u nd',\n",
              "   're n',\n",
              "   'c ess',\n",
              "   'ƒ† ent',\n",
              "   'ƒ†Y ou',\n",
              "   'ƒ†go od',\n",
              "   'ƒ†st art',\n",
              "   'in ce',\n",
              "   'ƒ†m ade',\n",
              "   't t',\n",
              "   'st em',\n",
              "   'ol og',\n",
              "   'u p',\n",
              "   'ƒ† |',\n",
              "   'um p',\n",
              "   'ƒ†he l',\n",
              "   'ver n',\n",
              "   'ul ar',\n",
              "   'u ally',\n",
              "   'ƒ†a c',\n",
              "   'ƒ†m on',\n",
              "   'ƒ†l ast',\n",
              "   'ƒ†2 00',\n",
              "   '1 0',\n",
              "   'ƒ†st ud',\n",
              "   'u res',\n",
              "   'ƒ†A r',\n",
              "   'sel f',\n",
              "   'ar s',\n",
              "   'mer ic',\n",
              "   'u es',\n",
              "   'c y',\n",
              "   'ƒ†m in',\n",
              "   'oll ow',\n",
              "   'ƒ†c ol',\n",
              "   'i o',\n",
              "   'ƒ†m od',\n",
              "   'ƒ†c ount',\n",
              "   'ƒ†C om',\n",
              "   'he s',\n",
              "   'ƒ†f in',\n",
              "   'a ir',\n",
              "   'i er',\n",
              "   '√¢ƒ¢ ƒ∂',\n",
              "   're ad',\n",
              "   'an k',\n",
              "   'at ch',\n",
              "   'e ver',\n",
              "   'ƒ†st r',\n",
              "   'ƒ†po int',\n",
              "   'or k',\n",
              "   'ƒ†N ew',\n",
              "   'ƒ†s ur',\n",
              "   'o ol',\n",
              "   'al k',\n",
              "   'em ent',\n",
              "   'ƒ†us ed',\n",
              "   'ra ct',\n",
              "   'we en',\n",
              "   'ƒ†s ame',\n",
              "   'ou n',\n",
              "   'ƒ†A l',\n",
              "   'c i',\n",
              "   'ƒ†diff ere',\n",
              "   'ƒ†wh ile',\n",
              "   '---- ----',\n",
              "   'ƒ†g ame',\n",
              "   'ce pt',\n",
              "   'ƒ†s im',\n",
              "   '.. .',\n",
              "   'ƒ†in ter',\n",
              "   'e k',\n",
              "   'ƒ†re port',\n",
              "   'ƒ†pro du',\n",
              "   'ƒ†st ill',\n",
              "   'l ed',\n",
              "   'a h',\n",
              "   'ƒ†he re',\n",
              "   'ƒ†wor ld',\n",
              "   'ƒ†th ough',\n",
              "   'ƒ†n um',\n",
              "   'ar ch',\n",
              "   'im es',\n",
              "   'al e',\n",
              "   'ƒ†S e',\n",
              "   'ƒ†I f',\n",
              "   '/ /',\n",
              "   'ƒ†L e',\n",
              "   'ƒ†re t',\n",
              "   'ƒ†re f',\n",
              "   'ƒ†tr ans',\n",
              "   'n er',\n",
              "   'ut ion',\n",
              "   'ter s',\n",
              "   'ƒ†t ake',\n",
              "   'ƒ†C l',\n",
              "   'ƒ†con f',\n",
              "   'w ay',\n",
              "   'a ve',\n",
              "   'ƒ†go ing',\n",
              "   'ƒ†s l',\n",
              "   'u g',\n",
              "   'ƒ†A meric',\n",
              "   'ƒ†spe c',\n",
              "   'ƒ†h and',\n",
              "   'ƒ†bet ween',\n",
              "   'ist s',\n",
              "   'ƒ†D e',\n",
              "   'o ot',\n",
              "   'I t',\n",
              "   'ƒ†e ar',\n",
              "   'ƒ†again st',\n",
              "   'ƒ†h igh',\n",
              "   'g an',\n",
              "   'a z',\n",
              "   'at her',\n",
              "   'ƒ†ex p',\n",
              "   'ƒ†o p',\n",
              "   'ƒ†in s',\n",
              "   'ƒ†g r',\n",
              "   'ƒ†hel p',\n",
              "   'ƒ†re qu',\n",
              "   'et s',\n",
              "   'in s',\n",
              "   'ƒ†P ro',\n",
              "   'is m',\n",
              "   'ƒ†f ound',\n",
              "   'l and',\n",
              "   'at a',\n",
              "   'us s',\n",
              "   'am es',\n",
              "   'ƒ†p erson',\n",
              "   'ƒ†g reat',\n",
              "   'p r',\n",
              "   'ƒ†s ign',\n",
              "   'ƒ†A n',\n",
              "   \"' ve\",\n",
              "   'ƒ†s omet',\n",
              "   'ƒ†s er',\n",
              "   'h ip',\n",
              "   'ƒ†r un',\n",
              "   'ƒ† :',\n",
              "   'ƒ†t er',\n",
              "   'ire ct',\n",
              "   'ƒ†f ollow',\n",
              "   'ƒ†d et',\n",
              "   'ic es',\n",
              "   'ƒ†f ind',\n",
              "   '1 2',\n",
              "   'ƒ†m em',\n",
              "   'ƒ†c r',\n",
              "   'e red',\n",
              "   'e x',\n",
              "   'ƒ†ex t',\n",
              "   'ut h',\n",
              "   'en se',\n",
              "   'c o',\n",
              "   'ƒ†te am',\n",
              "   'v ing',\n",
              "   'ou se',\n",
              "   'as h',\n",
              "   'at t',\n",
              "   'v ed',\n",
              "   'ƒ†sy stem',\n",
              "   'ƒ†A s',\n",
              "   'd er',\n",
              "   'iv es',\n",
              "   'm in',\n",
              "   'ƒ†le ad',\n",
              "   'ƒ†B l',\n",
              "   'c ent',\n",
              "   'ƒ†a round',\n",
              "   'ƒ†go vern',\n",
              "   'ƒ†c ur',\n",
              "   'vel op',\n",
              "   'an y',\n",
              "   'ƒ†c our',\n",
              "   'al th',\n",
              "   'ag es',\n",
              "   'iz e',\n",
              "   'ƒ†c ar',\n",
              "   'od e',\n",
              "   'ƒ†l aw',\n",
              "   'ƒ†re ad',\n",
              "   \"' m\",\n",
              "   'c on',\n",
              "   'ƒ†re al',\n",
              "   'ƒ†supp ort',\n",
              "   'ƒ†1 2',\n",
              "   '.. ..',\n",
              "   'ƒ†re ally',\n",
              "   'n ess',\n",
              "   'ƒ†f act',\n",
              "   'ƒ†d ay',\n",
              "   'ƒ†b oth',\n",
              "   'y ing',\n",
              "   'ƒ†s erv',\n",
              "   'ƒ†F or',\n",
              "   'ƒ†th ree',\n",
              "   'ƒ†w om',\n",
              "   'ƒ†m ed',\n",
              "   'od y',\n",
              "   'ƒ†The y',\n",
              "   '5 0',\n",
              "   'ƒ†ex per',\n",
              "   't on',\n",
              "   'ƒ†e ach',\n",
              "   'ak es',\n",
              "   'ƒ†c he',\n",
              "   'ƒ†c re',\n",
              "   'in es',\n",
              "   'ƒ†re p',\n",
              "   '1 9',\n",
              "   'g g',\n",
              "   'ill ion',\n",
              "   'ƒ†g rou',\n",
              "   'ut e',\n",
              "   'i k',\n",
              "   'W e',\n",
              "   'g et',\n",
              "   'E R',\n",
              "   'ƒ†m et',\n",
              "   'ƒ†s ays',\n",
              "   'o x',\n",
              "   'ƒ†d uring',\n",
              "   'er n',\n",
              "   'iz ed',\n",
              "   'a red',\n",
              "   'ƒ†f am',\n",
              "   'ic ally',\n",
              "   'ƒ†ha pp',\n",
              "   'ƒ†I s',\n",
              "   'ƒ†ch ar',\n",
              "   'm ed',\n",
              "   'v ent',\n",
              "   'ƒ†g ener',\n",
              "   'i ent',\n",
              "   'p le',\n",
              "   'i et',\n",
              "   're nt',\n",
              "   '1 1',\n",
              "   'v es',\n",
              "   'pt ion',\n",
              "   'ƒ†2 0',\n",
              "   'form ation',\n",
              "   'ƒ†c or',\n",
              "   'ƒ†off ic',\n",
              "   'ie ld',\n",
              "   'ƒ†to o',\n",
              "   'is ion',\n",
              "   'ƒ†in f',\n",
              "   'ƒ† Z',\n",
              "   't he',\n",
              "   'o ad',\n",
              "   'ƒ†p ublic',\n",
              "   'ƒ†pro g',\n",
              "   'r ic',\n",
              "   '* *',\n",
              "   'ƒ†w ar',\n",
              "   'ƒ†p ower',\n",
              "   'v iew',\n",
              "   'ƒ†f ew',\n",
              "   'ƒ†l oc',\n",
              "   'ƒ†differe nt',\n",
              "   'ƒ†st ate',\n",
              "   'ƒ†he ad',\n",
              "   \"' ll\",\n",
              "   'ƒ†p oss',\n",
              "   'ƒ†st at',\n",
              "   're t',\n",
              "   'ant s',\n",
              "   'ƒ†v al',\n",
              "   'ƒ†is s',\n",
              "   'ƒ†c le',\n",
              "   'i vers',\n",
              "   'an c',\n",
              "   'ƒ†ex pl',\n",
              "   'ƒ†an other',\n",
              "   'ƒ† Q',\n",
              "   'ƒ†a v',\n",
              "   'th ing',\n",
              "   'n ce',\n",
              "   'W h',\n",
              "   'ƒ†ch ild',\n",
              "   'ƒ†s ince',\n",
              "   'i red',\n",
              "   'l ess',\n",
              "   'ƒ†l ife',\n",
              "   'ƒ†de velop',\n",
              "   'itt le',\n",
              "   'ƒ†de p',\n",
              "   'ƒ†p ass',\n",
              "   '√£ ƒ•',\n",
              "   'ƒ†t urn',\n",
              "   'or n',\n",
              "   'Th is',\n",
              "   'b ers',\n",
              "   'ro ss',\n",
              "   'ƒ†A d',\n",
              "   'ƒ†f r',\n",
              "   'ƒ†res p',\n",
              "   'ƒ†sec ond',\n",
              "   'o h',\n",
              "   'ƒ† /',\n",
              "   'ƒ†dis c',\n",
              "   'ƒ† &',\n",
              "   'ƒ†somet hing',\n",
              "   'ƒ†comp le',\n",
              "   'ƒ† ed',\n",
              "   'ƒ†f il',\n",
              "   'ƒ†mon th',\n",
              "   'a j',\n",
              "   'u c',\n",
              "   'ƒ†govern ment',\n",
              "   'ƒ†with out',\n",
              "   'ƒ†le g',\n",
              "   'ƒ†d ist',\n",
              "   'ƒ†p ut',\n",
              "   'ƒ†qu est',\n",
              "   'an n',\n",
              "   'ƒ†pro t',\n",
              "   '2 0',\n",
              "   'ƒ†ne ver',\n",
              "   'i ence',\n",
              "   'ƒ†le vel',\n",
              "   'ƒ†ar t',\n",
              "   'ƒ†th ings',\n",
              "   'ƒ†m ight',\n",
              "   'ƒ†eff ect',\n",
              "   'ƒ†cont ro',\n",
              "   'ƒ†c ent',\n",
              "   'ƒ†1 8',\n",
              "   'ƒ†all ow',\n",
              "   'ƒ†bel ie',\n",
              "   'ch ool',\n",
              "   'ot t',\n",
              "   'ƒ†inc re',\n",
              "   'ƒ†fe el',\n",
              "   'ƒ†res ult',\n",
              "   ...]}}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer_json['version']\n",
        "#tokenizer['model']\n",
        "#tokenizer['model']['vocab']\n",
        "tokenizer_json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How these files are used  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of text: 52\n",
            "A day is just better with Lavazza coffee. You agree?\n"
          ]
        }
      ],
      "source": [
        "text = 'A day is just better with Lavazza coffee. You agree?'\n",
        "print('length of text:',len(text))\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1.** First step in the tokenizer is to tokenize according to the merges-file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of tokens: 14\n",
            "tokens from pre-trained:\n",
            " ['A', 'ƒ†day', 'ƒ†is', 'ƒ†just', 'ƒ†better', 'ƒ†with', 'ƒ†Lav', 'az', 'za', 'ƒ†coffee', '.', 'ƒ†You', 'ƒ†agree', '?']\n",
            "tokens from checkpoint:\n",
            " ['A', 'ƒ†day', 'ƒ†is', 'ƒ†just', 'ƒ†better', 'ƒ†with', 'ƒ†Lav', 'az', 'za', 'ƒ†coffee', '.', 'ƒ†You', 'ƒ†agree', '?']\n"
          ]
        }
      ],
      "source": [
        "print('number of tokens:', len(tokenizer_pretrained.tokenize(text)))\n",
        "print('tokens from pre-trained:\\n',tokenizer_pretrained.tokenize(text))\n",
        "print('tokens from checkpoint:\\n',tokenizer_checkpoint.tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*ƒ† is the rep. for space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2.** Second step is to replace these tokens with their corresponding indices, using the vocab-file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of idx: 16\n",
            "indices from pre-trained:\n",
            " [0, 250, 183, 16, 95, 357, 19, 18126, 1222, 2478, 3895, 4, 370, 2854, 116, 2]\n",
            "indices from checkpoint:\n",
            " [0, 250, 183, 16, 95, 357, 19, 18126, 1222, 2478, 3895, 4, 370, 2854, 116, 2]\n"
          ]
        }
      ],
      "source": [
        "print('number of idx:',len(tokenizer_pretrained.encode(text)))\n",
        "print('indices from pre-trained:\\n',tokenizer_pretrained.encode(text))\n",
        "print('indices from checkpoint:\\n',tokenizer_checkpoint.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Where the following for end and start of sentence is used \n",
        "* sep : ['< /s>', 2] (last token of a sequence built with special tokens)\n",
        "* cls : ['< s>', 0] (fisrt token of a sequence built with special tokens)\n",
        "\n",
        "Having the indices we can decode back to original text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decode from pre-trained:\n",
            " <s>A day is just better with Lavazza coffee. You agree?</s>\n",
            "decode from checkpoint:\n",
            " <s>A day is just better with Lavazza coffee. You agree?</s>\n"
          ]
        }
      ],
      "source": [
        "print('decode from pre-trained:\\n',tokenizer_pretrained.decode(tokenizer_pretrained.encode(text)))\n",
        "print('decode from checkpoint:\\n',tokenizer_checkpoint.decode(tokenizer_checkpoint.encode(text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dict.txt file is the connection between GPT-2 vocab and RoBERTa vocab. \n",
        "\n",
        "* Where the **row-idx+4 is the index in RoBERTa** - the 4 is from the 4 special tokens (see below).\n",
        "* And the column 'index' is the **index from GPT-2.**\n",
        "* The column 'occurencies' gives the number of times the **index/token appears**' in the training set. \n",
        "\n",
        "The GPT-2 vocab is remapped with the RoBERTa vocab and the first four values are the special tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<s>': 0, '<pad>': 1, '</s>': 2, '<unk>': 3}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# special tokens\n",
        "{\"<s>\": 0, \"<pad>\": 1, \"</s>\": 2, \"<unk>\": 3}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Settings for Tokenizer\n",
        "Tokenizer.json gives all settings for the Tokenizer.\n",
        "\n",
        "As examples are that it gives which special tokens are added and their corresponding id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 0, 'special': True, 'content': '<s>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': True}\n",
            "{'id': 1, 'special': True, 'content': '<pad>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': True}\n",
            "{'id': 2, 'special': True, 'content': '</s>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': True}\n",
            "{'id': 3, 'special': True, 'content': '<unk>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': True}\n",
            "{'id': 50264, 'special': True, 'content': '<mask>', 'single_word': False, 'lstrip': True, 'rstrip': False, 'normalized': True}\n"
          ]
        }
      ],
      "source": [
        "nr_add_tokens = len(tokenizer_json['added_tokens'])\n",
        "for i in range(nr_add_tokens):\n",
        "    print(tokenizer_json['added_tokens'][i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Where **cls** is the classifier token which is used when doing sequence classification (classification of the whole sequence instead of per-token classification)\n",
        "\n",
        "and **sep** is the separator token, which is used when building a sequence from multiple sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'type': 'RobertaProcessing',\n",
              " 'sep': ['</s>', 2],\n",
              " 'cls': ['<s>', 0],\n",
              " 'trim_offsets': True,\n",
              " 'add_prefix_space': False}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer_json['post_processor']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Some of the inputs to the Tokenizer\n",
        "\n",
        "* **bos_token** (str, optional, defaults to \"< s>\") ‚Äî The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n",
        "\n",
        "* **eos_token** (str, optional, defaults to \"< /s>\") ‚Äî The end of sequence token.\n",
        "\n",
        "* **sep_token** (str, optional, defaults to \"< /s>\") ‚Äî The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for sequence classification or for a text and a question for question answering. It is also used as the last token of a sequence built with special tokens.\n",
        "\n",
        "* **cls_token** (str, optional, defaults to \"< s>\") ‚Äî The classifier token which is used when doing sequence classification (classification of the whole sequence instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
        "\n",
        "* **unk_token** (str, optional, defaults to \"< unk>\") ‚Äî The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this token instead.\n",
        "\n",
        "* **pad_token** (str, optional, defaults to \"< pad>\") ‚Äî The token used for padding, for example when batching sequences of different lengths.\n",
        "\n",
        "* **mask_token** (str, optional, defaults to \"< mask>\") ‚Äî The token used for masking values. This is the token used when training this model with masked language modeling. This is the token which the model will try to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prediction \n",
        "A little bit of prediction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:01<00:01,  1.06s/ba]\n"
          ]
        }
      ],
      "source": [
        "# Prepare the text inputs for the model\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer_checkpoint(examples[\"sentence\"], truncation=True)\n",
        "\n",
        "tokenized_test = test_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                        \n",
        "    tokenizer=tokenizer_checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 1821\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Predicting with model\n",
        "predictions = trainer.predict(tokenized_test)\n",
        "#dataset_test_pred = list(np.argmax(predictions.predictions, axis=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result/output shape (1821, 2)\n",
            "hidden layer shape (1821, 55, 768)\n",
            "number of hidden layers 13\n"
          ]
        }
      ],
      "source": [
        "print('result/output shape',predictions.predictions[0].shape)\n",
        "print('hidden layer shape',predictions.predictions[1][0].shape)\n",
        "print('number of hidden layers',len(predictions.predictions[1]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Number of hidden layers**: one for the output of the embeddings, if the model has an embedding layer, + one for the output of each layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentence hidden shape: (55, 768)\n",
            "sentence embedding shape: (55,)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([ 4.8460227e-02,  4.3200690e-02,  3.5231877e-02,  3.5319056e-02,\n",
              "        3.5543062e-02,  2.8869808e-02,  3.3768181e-02,  3.3345446e-02,\n",
              "        3.3249509e-02,  4.1137930e-02,  4.8487872e-02,  3.1430356e-02,\n",
              "        3.1430356e-02,  3.1430356e-02,  3.1430356e-02,  3.1430356e-02,\n",
              "        3.1430356e-02,  3.1430356e-02,  3.1430356e-02,  3.1430356e-02,\n",
              "        3.1430356e-02,  3.1430356e-02,  3.1430356e-02,  3.1430356e-02,\n",
              "        3.1430356e-02,  3.1430356e-02,  3.1430356e-02,  3.1430356e-02,\n",
              "        3.1430356e-02,  3.1430356e-02,  3.1430375e-02,  3.1430375e-02,\n",
              "        3.1430375e-02, -1.0000000e+02, -1.0000000e+02, -1.0000000e+02,\n",
              "       -1.0000000e+02, -1.0000000e+02, -1.0000000e+02, -1.0000000e+02,\n",
              "       -1.0000000e+02, -1.0000000e+02, -1.0000000e+02, -1.0000000e+02,\n",
              "       -1.0000000e+02, -1.0000000e+02, -1.0000000e+02, -1.0000000e+02,\n",
              "       -1.0000000e+02, -1.0000000e+02, -1.0000000e+02, -1.0000000e+02,\n",
              "       -1.0000000e+02, -1.0000000e+02, -1.0000000e+02], dtype=float32)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_vec = predictions.predictions[1][-1][0]\n",
        "print('sentence hidden shape:', token_vec.shape)\n",
        "sentence_embedding = np.mean(token_vec, axis = 1)\n",
        "# or \n",
        "# sentence_embedding = np.mean(token_vec, axis = 0)\n",
        "print('sentence embedding shape:', sentence_embedding.shape)\n",
        "sentence_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[ 1.62484393e-01, -4.84118685e-02, -1.51357776e-03, ...,\n",
              "         -8.22553486e-02,  8.29455554e-02,  1.51815377e-02],\n",
              "        [ 9.36164483e-02,  1.50938453e-02, -2.52682511e-02, ...,\n",
              "         -4.28227901e-01,  1.36146918e-01,  2.34315187e-01],\n",
              "        [-2.59152967e-02,  1.19985595e-01,  5.50623424e-02, ...,\n",
              "          1.40090331e-01,  3.86071503e-02, -5.08246496e-02],\n",
              "        ...,\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02],\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02],\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02]],\n",
              "\n",
              "       [[ 1.62484393e-01, -4.84118685e-02, -1.51357776e-03, ...,\n",
              "         -8.22553486e-02,  8.29455554e-02,  1.51815377e-02],\n",
              "        [ 4.33467209e-01,  1.63404524e-01, -5.55436388e-02, ...,\n",
              "         -3.11418530e-03, -5.23298271e-02, -1.00343436e-01],\n",
              "        [ 2.62533456e-01,  1.20133445e-01,  2.08018571e-02, ...,\n",
              "         -1.22748695e-01,  1.37995481e-01,  4.10862006e-02],\n",
              "        ...,\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02],\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02],\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02]],\n",
              "\n",
              "       [[ 1.62484393e-01, -4.84118685e-02, -1.51357776e-03, ...,\n",
              "         -8.22553486e-02,  8.29455554e-02,  1.51815377e-02],\n",
              "        [-6.42175138e-01, -7.18313549e-03,  2.31940627e-01, ...,\n",
              "         -7.07854331e-01, -3.55922997e-01,  3.82496506e-01],\n",
              "        [-5.49801700e-02,  6.28612280e-01, -4.52599496e-01, ...,\n",
              "         -2.81976044e-01, -4.35780108e-01, -1.08495295e-01],\n",
              "        ...,\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02],\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02],\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 1.62484393e-01, -4.84118685e-02, -1.51357776e-03, ...,\n",
              "         -8.22553486e-02,  8.29455554e-02,  1.51815377e-02],\n",
              "        [-2.79730797e-01, -2.37394229e-01,  3.31820101e-01, ...,\n",
              "         -6.06137468e-03, -4.12461787e-01,  1.06701367e-01],\n",
              "        [-1.33822439e-02, -7.77012855e-02,  8.14860985e-02, ...,\n",
              "         -6.90349936e-02, -6.44419432e-01, -9.80728641e-02],\n",
              "        ...,\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02],\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02],\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02]],\n",
              "\n",
              "       [[ 1.62484393e-01, -4.84118685e-02, -1.51357776e-03, ...,\n",
              "         -8.22553486e-02,  8.29455554e-02,  1.51815377e-02],\n",
              "        [ 5.14573336e-01, -2.72488326e-01,  6.17331229e-02, ...,\n",
              "         -2.08112061e-01, -1.07009105e-01,  5.36732078e-01],\n",
              "        [ 1.42188117e-01, -2.53821850e-01, -8.82796869e-02, ...,\n",
              "          2.72820950e-01,  1.24757141e-01,  3.28613907e-01],\n",
              "        ...,\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02],\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02],\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02]],\n",
              "\n",
              "       [[ 1.62484393e-01, -4.84118685e-02, -1.51357776e-03, ...,\n",
              "         -8.22553486e-02,  8.29455554e-02,  1.51815377e-02],\n",
              "        [ 1.71293661e-01,  8.48021451e-03, -2.79158563e-03, ...,\n",
              "         -4.66371804e-01, -3.40826690e-01, -8.29145089e-02],\n",
              "        [ 1.85336545e-01, -2.03477994e-01,  3.28076035e-01, ...,\n",
              "          3.99490148e-01, -1.85412504e-02,  4.53915708e-02],\n",
              "        ...,\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02],\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02],\n",
              "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
              "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02]]],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions.predictions[1][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#accuracy_metric = load_metric(\"accuracy\")\n",
        "#accuracy_metric.compute(predictions=, references=test_dataset['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Some other text examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_text = [ \"It tokenize rare english words in multiple: Obelus, Nudiustertian, Nikehedonia and Metanoia\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tokenizer first tokenizes according to the merges file:\n",
            " ['It', 'ƒ†token', 'ize', 'ƒ†rare', 'ƒ†english', 'ƒ†words', 'ƒ†in', 'ƒ†multiple', ':', 'ƒ†Ob', 'el', 'us', ',', 'ƒ†N', 'udi', 'ust', 'ert', 'ian', ',', 'ƒ†Nike', 'hed', 'onia', 'ƒ†and', 'ƒ†Met', 'anoia']\n",
            "And then(according to the values in the vocab.json)\n",
            "these tokens are then replaced by their indices:\n",
            " [0, 243, 19233, 2072, 3159, 47510, 1617, 11, 1533, 35, 5816, 523, 687, 6, 234, 24110, 4193, 2399, 811, 6, 10239, 4183, 15402, 8, 4369, 47733, 2]\n",
            "<s>It tokenize rare english words in multiple: Obelus, Nudiustertian, Nikehedonia and Metanoia</s>\n",
            "['<s>', 'It', 'ƒ†token', 'ize', 'ƒ†rare', 'ƒ†english', 'ƒ†words', 'ƒ†in', 'ƒ†multiple', ':', 'ƒ†Ob', 'el', 'us', ',', 'ƒ†N', 'udi', 'ust', 'ert', 'ian', ',', 'ƒ†Nike', 'hed', 'onia', 'ƒ†and', 'ƒ†Met', 'anoia', '</s>']\n"
          ]
        }
      ],
      "source": [
        "text = test_text\n",
        "for i in range(len(text)):\n",
        "    ids = tokenizer_pretrained(text[i], truncation=True)['input_ids']\n",
        "    \n",
        "    print('The tokenizer first tokenizes according to the merges file:\\n',tokenizer_pretrained.tokenize(text[i]))\n",
        "    print('And then(according to the values in the vocab.json)\\nthese tokens are then replaced by their indices:\\n',ids)\n",
        "\n",
        "    print(tokenizer_pretrained.decode(ids))\n",
        "    print(tokenizer_pretrained.convert_ids_to_tokens(ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# input_ids: the token indices\n",
        "# attention_mask: exactly ehat it says - a 0 or 1 array that tells the model which tokens should be attended to and which should not\n",
        "\n",
        "# The truncation argument controls truncation. It can be a boolean or a string:\n",
        "# True or 'longest_first': \n",
        "# truncate to a maximum length specified by the max_length argument or the maximum length accepted by the model if no max_length is provided (max_length=None). \n",
        "# This will truncate token by token, removing a token from the longest sequence in the pair until the proper length is reached."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [[0, 37265, 92, 3556, 2485, 31, 5, 20536, 2833, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 6025, 6138, 63, 3768, 8, 39906, 402, 1195, 2721, 59, 1050, 2574, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 10800, 5069, 117, 22094, 2156, 129, 6348, 3995, 821, 8299, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 34084, 6031, 1626, 14, 5, 736, 9, 215, 1368, 9718, 1803, 32353, 25, 36409, 426, 64, 202, 1004, 66, 10, 650, 2156, 1081, 822, 19, 41, 3722, 2204, 1517, 479, 1437, 2], [0, 5593, 5069, 19223, 10028, 7, 1091, 5, 276, 1328, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer_pretrained(test_dataset['sentence'][:5], padding=True,truncation = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPYLcMM00AhZ"
      },
      "source": [
        "We then map the tokenizer over our dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], truncation=True)\n",
        "\n",
        "dataset = load_dataset(\"sst2\")\n",
        "\n",
        "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select([i for i in list(range(100))])\n",
        "small_val_dataset = dataset[\"validation\"].shuffle(seed=42).select([i for i in list(range(50))])\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_val = small_val_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "#model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('roberta_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2c3ec90920587dcd62ca10f98568309ae5fe8dd1757bd16b3e1a83d20ad0c067"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
