{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5CYHcPO0AhL"
      },
      "source": [
        "# Visualize your ü§ó Hugging Face data\n",
        "#### üõ†Ô∏è Installation and set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "JurK8mIH0AhP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from transformers import RobertaTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVXUoDKy0AhQ"
      },
      "source": [
        "### üõ´ Data and model preparation\n",
        "#### üè∑Ô∏è Loading a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "quwoNjNt0AhR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset sst2 (/zhome/94/5/127021/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 377.21it/s]\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"sst2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['idx', 'sentence', 'label'],\n",
              "        num_rows: 67349\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['idx', 'sentence', 'label'],\n",
              "        num_rows: 872\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['idx', 'sentence', 'label'],\n",
              "        num_rows: 1821\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For demo sub-sample dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aM29Dj760AhX"
      },
      "outputs": [],
      "source": [
        "small_data_train = dataset['train'].select(range(dataset['train'].num_rows // 10))\n",
        "# alternative methods\n",
        "# dataset[\"train\"].shuffle(seed=42).select([i for i in list(range(100))])\n",
        "small_data_val = dataset['validation'].select(range(dataset['validation'].num_rows // 10)) # dataset[\"validation\"].shuffle(seed=42).select([i for i in list(range(50))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9lYU6nB0AhX"
      },
      "source": [
        "### ‚öôÔ∏è Tokenizing the dataset\n",
        "In a typical NLP workflow, we must first tokenize our dataset.\n",
        "\n",
        "Converting the stream of characters in the text into a stream of defined \"tokens\", which can be anything from a smaller set of characters to words from a vocabulary.\n",
        "\n",
        "We will use a pretrained model, so we inherit its tokenization scheme.\n",
        "\n",
        "Wanting to see all files on RoBERTa e.g. tokenization https://huggingface.co/roberta-base/tree/main \n",
        "\n",
        "**Merge**-file explanation https://github.com/huggingface/transformers/issues/4777 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Files used for the RoBERTa pre-trained Tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current wokring directory /zhome/94/5/127021/speciale/master_project/notebooks\n"
          ]
        }
      ],
      "source": [
        "ellen_little_nb_path = '/zhome/94/5/127021/speciale/master_project/notebooks'\n",
        "roberta_files_path = '/work3/s174498/roberta_files/'\n",
        "print('Current wokring directory',os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Change cwd /work3/s174498/roberta_files\n",
            "New cwd /zhome/94/5/127021/speciale/master_project/notebooks\n"
          ]
        }
      ],
      "source": [
        "os.chdir(roberta_files_path)\n",
        "print('Change cwd',os.getcwd())\n",
        "df_merges = pd.read_csv(\"merges.txt\", sep=\" \",  on_bad_lines='skip')\n",
        "df_dict = pd.read_csv(\"dict.txt\", sep=\" \", header = None, names = ['id_GPT_2','occurrence'])\n",
        "# df_merges = pd.read_csv(\"merges.txt\", sep=\" \")\n",
        "file = open('tokenizer.json')\n",
        "tokenizer_json = json.load(file)\n",
        "file.close()\n",
        "file = open('vocab.json')\n",
        "vocab = json.load(file)\n",
        "file.close()\n",
        "os.chdir(ellen_little_nb_path)\n",
        "print('New cwd',os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_GPT_2</th>\n",
              "      <th>occurrence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13</td>\n",
              "      <td>850314647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>262</td>\n",
              "      <td>800385005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "      <td>800251374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>284</td>\n",
              "      <td>432911125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>290</td>\n",
              "      <td>394899794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50255</th>\n",
              "      <td>50009</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50256</th>\n",
              "      <td>50256</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50257</th>\n",
              "      <td>madeupword0000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50258</th>\n",
              "      <td>madeupword0001</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50259</th>\n",
              "      <td>madeupword0002</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50260 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             id_GPT_2  occurrence\n",
              "0                  13   850314647\n",
              "1                 262   800385005\n",
              "2                  11   800251374\n",
              "3                 284   432911125\n",
              "4                 290   394899794\n",
              "...               ...         ...\n",
              "50255           50009           0\n",
              "50256           50256           0\n",
              "50257  madeupword0000           0\n",
              "50258  madeupword0001           0\n",
              "50259  madeupword0002           0\n",
              "\n",
              "[50260 rows x 2 columns]"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Vocab** consists of 50265 'units'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab length: 50265\n",
            "the 10 first: ['<s>', '<pad>', '</s>', '<unk>', '.', 'ƒ†the', ',', 'ƒ†to', 'ƒ†and', 'ƒ†of']\n"
          ]
        }
      ],
      "source": [
        "print('vocab length:', len(vocab.keys()))\n",
        "print('the 10 first:',list(vocab.keys())[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tokenizer** has a lot of information about model, and which setting are chosen and the vocab can be found here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1.0'"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer_json['version']\n",
        "#tokenizer['model']\n",
        "#tokenizer['model']['vocab']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How these files are used  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of text: 52\n",
            "A day is just better with Lavazza coffee. You agree?\n"
          ]
        }
      ],
      "source": [
        "text = 'A day is just better with Lavazza coffee. You agree?'\n",
        "print('length of text:',len(text))\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1.** First step in the tokenizer is to tokenize according to the merges-file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of tokens: 14\n",
            "['A', 'ƒ†day', 'ƒ†is', 'ƒ†just', 'ƒ†better', 'ƒ†with', 'ƒ†Lav', 'az', 'za', 'ƒ†coffee', '.', 'ƒ†You', 'ƒ†agree', '?']\n"
          ]
        }
      ],
      "source": [
        "print('number of tokens:', len(tokenizer.tokenize(text)))\n",
        "print(tokenizer.tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*ƒ† is the rep. for space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2.** Second step is to replace these tokens with their corresponding indices, using the vocab-file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of idx: 16\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0, 250, 183, 16, 95, 357, 19, 18126, 1222, 2478, 3895, 4, 370, 2854, 116, 2]"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print('number of idx:',len(tokenizer.encode(text)))\n",
        "tokenizer.encode(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Where the following for end and start of sentence is used \n",
        "* sep : ['< /s>', 2] (last token of a sequence built with special tokens)\n",
        "* cls : ['< s>', 0] (fisrt token of a sequence built with special tokens)\n",
        "\n",
        "Having the indices we can decode back to original text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<s>A day is just better with Lavazza coffee. You agree?</s>'"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dict.txt file is the connection between GPT-2 vocab and RoBERTa vocab. \n",
        "\n",
        "* Where the **row-idx+4 is the index in RoBERTa** - the 4 is from the 4 special tokens (see below).\n",
        "* And the column 'index' is the **index from GPT-2.**\n",
        "* The column 'occurencies' gives the number of times the **index/token appears**' in the training set. \n",
        "\n",
        "The GPT-2 vocab is remapped with the RoBERTa vocab and the first four values are the special tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<s>': 0, '<pad>': 1, '</s>': 2, '<unk>': 3}"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# special tokens\n",
        "{\"<s>\": 0, \"<pad>\": 1, \"</s>\": 2, \"<unk>\": 3}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Settings for Tokenizer\n",
        "Tokenizer.json gives all settings for the Tokenizer.\n",
        "\n",
        "As examples are that it gives which special tokens are added and their corresponding id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 0, 'special': True, 'content': '<s>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': True}\n",
            "{'id': 1, 'special': True, 'content': '<pad>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': True}\n",
            "{'id': 2, 'special': True, 'content': '</s>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': True}\n",
            "{'id': 3, 'special': True, 'content': '<unk>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': True}\n",
            "{'id': 50264, 'special': True, 'content': '<mask>', 'single_word': False, 'lstrip': True, 'rstrip': False, 'normalized': True}\n"
          ]
        }
      ],
      "source": [
        "nr_add_tokens = len(tokenizer_json['added_tokens'])\n",
        "for i in range(nr_add_tokens):\n",
        "    print(tokenizer_json['added_tokens'][i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Where **cls** is the classifier token which is used when doing sequence classification (classification of the whole sequence instead of per-token classification)\n",
        "\n",
        "and **sep** is the separator token, which is used when building a sequence from multiple sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'type': 'RobertaProcessing',\n",
              " 'sep': ['</s>', 2],\n",
              " 'cls': ['<s>', 0],\n",
              " 'trim_offsets': True,\n",
              " 'add_prefix_space': False}"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer_json['post_processor']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Some of the inputs to the Tokenizer\n",
        "\n",
        "* **bos_token** (str, optional, defaults to \"< s>\") ‚Äî The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n",
        "\n",
        "* **eos_token** (str, optional, defaults to \"< /s>\") ‚Äî The end of sequence token.\n",
        "\n",
        "* **sep_token** (str, optional, defaults to \"< /s>\") ‚Äî The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for sequence classification or for a text and a question for question answering. It is also used as the last token of a sequence built with special tokens.\n",
        "\n",
        "* **cls_token** (str, optional, defaults to \"< s>\") ‚Äî The classifier token which is used when doing sequence classification (classification of the whole sequence instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
        "\n",
        "* **unk_token** (str, optional, defaults to \"< unk>\") ‚Äî The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this token instead.\n",
        "\n",
        "* **pad_token** (str, optional, defaults to \"< pad>\") ‚Äî The token used for padding, for example when batching sequences of different lengths.\n",
        "\n",
        "* **mask_token** (str, optional, defaults to \"< mask>\") ‚Äî The token used for masking values. This is the token used when training this model with masked language modeling. This is the token which the model will try to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_text = [\"Hello World, Hello World, and hello world differs.\", \"It tokenize Danish words and rare english words such as 'speciale skrivning' and Obelus, Nudiustertian, Nikehedonia and Metanoia\"]\n",
        "test_text2 = ['in store and dog',' in store ','in store ',' in store','in store?','in store.','in store .']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tokenizer first tokenizes according to the merges file:\n",
            " ['Hello', 'ƒ†World', ',', 'ƒ†Hello', 'ƒ†World', ',', 'ƒ†and', 'ƒ†hello', 'ƒ†world', 'ƒ†differs', '.']\n",
            "And then(according to the values in the vocab.json)\n",
            "these tokens are then replaced by their indices:\n",
            " [0, 31414, 623, 6, 20920, 623, 6, 8, 20760, 232, 31381, 4, 2]\n",
            "<s>Hello World, Hello World, and hello world differs.</s>\n",
            "['<s>', 'Hello', 'ƒ†World', ',', 'ƒ†Hello', 'ƒ†World', ',', 'ƒ†and', 'ƒ†hello', 'ƒ†world', 'ƒ†differs', '.', '</s>']\n",
            "The tokenizer first tokenizes according to the merges file:\n",
            " ['It', 'ƒ†token', 'ize', 'ƒ†Danish', 'ƒ†words', 'ƒ†and', 'ƒ†rare', 'ƒ†english', 'ƒ†words', 'ƒ†such', 'ƒ†as', \"ƒ†'\", 'special', 'e', 'ƒ†sk', 'riv', 'ning', \"'\", 'ƒ†and', 'ƒ†Ob', 'el', 'us', ',', 'ƒ†N', 'udi', 'ust', 'ert', 'ian', ',', 'ƒ†Nike', 'hed', 'onia', 'ƒ†and', 'ƒ†Met', 'anoia']\n",
            "And then(according to the values in the vocab.json)\n",
            "these tokens are then replaced by their indices:\n",
            " [0, 243, 19233, 2072, 13501, 1617, 8, 3159, 47510, 1617, 215, 25, 128, 19423, 242, 2972, 16936, 3509, 108, 8, 5816, 523, 687, 6, 234, 24110, 4193, 2399, 811, 6, 10239, 4183, 15402, 8, 4369, 47733, 2]\n",
            "<s>It tokenize Danish words and rare english words such as'speciale skrivning' and Obelus, Nudiustertian, Nikehedonia and Metanoia</s>\n",
            "['<s>', 'It', 'ƒ†token', 'ize', 'ƒ†Danish', 'ƒ†words', 'ƒ†and', 'ƒ†rare', 'ƒ†english', 'ƒ†words', 'ƒ†such', 'ƒ†as', \"ƒ†'\", 'special', 'e', 'ƒ†sk', 'riv', 'ning', \"'\", 'ƒ†and', 'ƒ†Ob', 'el', 'us', ',', 'ƒ†N', 'udi', 'ust', 'ert', 'ian', ',', 'ƒ†Nike', 'hed', 'onia', 'ƒ†and', 'ƒ†Met', 'anoia', '</s>']\n"
          ]
        }
      ],
      "source": [
        "text = test_text\n",
        "for i in range(len(text)):\n",
        "    ids = tokenizer(text[i], truncation=True)['input_ids']\n",
        "    \n",
        "    print('The tokenizer first tokenizes according to the merges file:\\n',tokenizer.tokenize(text[i]))\n",
        "    print('And then(according to the values in the vocab.json)\\nthese tokens are then replaced by their indices:\\n',ids)\n",
        "\n",
        "    print(tokenizer.decode(ids))\n",
        "    print(tokenizer.convert_ids_to_tokens(ids))\n",
        "\n",
        "#tokenizer(test_text2, truncation = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# input_ids: the token indices\n",
        "# attention_mask: exactly ehat it says - a 0 or 1 array that tells the model which tokens should be attended to and which should not\n",
        "\n",
        "# The truncation argument controls truncation. It can be a boolean or a string:\n",
        "# True or 'longest_first': \n",
        "# truncate to a maximum length specified by the max_length argument or the maximum length accepted by the model if no max_length is provided (max_length=None). \n",
        "# This will truncate token by token, removing a token from the longest sequence in the pair until the proper length is reached."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [[0, 37265, 92, 3556, 2485, 31, 5, 20536, 2833, 1437, 2], [0, 10800, 5069, 117, 22094, 2156, 129, 6348, 3995, 821, 8299, 1437, 2], [0, 6025, 6138, 63, 3768, 8, 39906, 402, 1195, 2721, 59, 1050, 2574, 1437, 2], [0, 5593, 5069, 19223, 10028, 7, 1091, 5, 276, 1328, 1437, 2], [0, 261, 5, 2373, 13543, 12, 1116, 12, 627, 12, 1396, 11622, 43848, 5739, 5, 17504, 115, 31120, 1899, 62, 1437, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(small_data_train['sentence'][:5])#, truncation = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPYLcMM00AhZ"
      },
      "source": [
        "We then map the tokenizer over our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n",
            "Downloading and preparing dataset sst2/default (download: 7.09 MiB, generated: 4.78 MiB, post-processed: Unknown size, total: 11.88 MiB) to /zhome/94/5/127021/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5...\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "[Errno 28] No space left on device: '/zhome/94/5/127021/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5.incomplete'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_function\u001b[39m(examples):\n\u001b[1;32m      2\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(examples[\u001b[39m\"\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m\"\u001b[39m], truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39msst2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m small_train_dataset \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshuffle(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\u001b[39m.\u001b[39mselect([i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m))])\n\u001b[1;32m      7\u001b[0m small_val_dataset \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshuffle(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\u001b[39m.\u001b[39mselect([i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m))])\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/datasets/load.py:1742\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1739\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1741\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1742\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   1743\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1744\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1745\u001b[0m     ignore_verifications\u001b[39m=\u001b[39;49mignore_verifications,\n\u001b[1;32m   1746\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   1747\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1748\u001b[0m )\n\u001b[1;32m   1750\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   1752\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   1753\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/datasets/builder.py:793\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_manual_download(dl_manager)\n\u001b[1;32m    792\u001b[0m \u001b[39m# Create a tmp dir and rename to self._output_dir on successful exit.\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m \u001b[39mwith\u001b[39;00m incomplete_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir) \u001b[39mas\u001b[39;00m tmp_output_dir:\n\u001b[1;32m    794\u001b[0m     \u001b[39m# Temporarily assign _output_dir to tmp_data_dir to avoid having to forward\u001b[39;00m\n\u001b[1;32m    795\u001b[0m     \u001b[39m# it to every sub function.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m     \u001b[39mwith\u001b[39;00m temporary_assignment(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_output_dir\u001b[39m\u001b[39m\"\u001b[39m, tmp_output_dir):\n\u001b[1;32m    797\u001b[0m \n\u001b[1;32m    798\u001b[0m         \u001b[39m# Try to download the already prepared dataset files\u001b[39;00m\n\u001b[1;32m    799\u001b[0m         downloaded_from_gcs \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    120\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/datasets/builder.py:763\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare.<locals>.incomplete_dir\u001b[0;34m(dirname)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     tmp_dir \u001b[39m=\u001b[39m dirname \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.incomplete\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 763\u001b[0m     os\u001b[39m.\u001b[39;49mmakedirs(tmp_dir, exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    764\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m         \u001b[39myield\u001b[39;00m tmp_dir\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     mkdir(name, mode)\n\u001b[1;32m    226\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '/zhome/94/5/127021/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5.incomplete'"
          ]
        }
      ],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], truncation=True)\n",
        "\n",
        "dataset = load_dataset(\"sst2\")\n",
        "\n",
        "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select([i for i in list(range(100))])\n",
        "small_val_dataset = dataset[\"validation\"].shuffle(seed=42).select([i for i in list(range(50))])\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_val = small_val_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "#model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('roberta_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2c3ec90920587dcd62ca10f98568309ae5fe8dd1757bd16b3e1a83d20ad0c067"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
