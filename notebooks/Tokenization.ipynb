{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5CYHcPO0AhL"
      },
      "source": [
        "# Visualize your ü§ó Hugging Face data\n",
        "#### üõ†Ô∏è Installation and set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JurK8mIH0AhP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/zhome/94/5/127021/miniconda3/envs/roberta_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer\n",
        "import torch\n",
        "from datasets import load_from_disk, load_metric, Dataset, load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVXUoDKy0AhQ"
      },
      "source": [
        "### üõ´ Data and model preparation\n",
        "#### üè∑Ô∏è Loading a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "quwoNjNt0AhR"
      },
      "outputs": [],
      "source": [
        "# load\n",
        "datadir = '/work3/s174498/sst2_dataset/'\n",
        "\n",
        "from_disk = True\n",
        "all = False\n",
        "\n",
        "if from_disk:\n",
        "    test_dataset = load_from_disk(datadir + 'test_dataset')\n",
        "    if all:\n",
        "        train_dataset = load_from_disk(datadir + 'train_dataset')\n",
        "        validation_dataset = load_from_disk(datadir + 'validation_dataset')\n",
        "else:\n",
        "    if all:\n",
        "        dataset = load_dataset(\"sst2\")\n",
        "        train_dataset = dataset['train']\n",
        "        validation_dataset = dataset['validation']\n",
        "        test_dataset = dataset['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For demo sub-sample dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aM29Dj760AhX"
      },
      "outputs": [],
      "source": [
        "#small_data_train = dataset['train'].select(range(dataset['train'].num_rows // 10))\n",
        "# alternative methods\n",
        "# dataset[\"train\"].shuffle(seed=42).select([i for i in list(range(100))])\n",
        "#small_data_val = dataset['validation'].select(range(dataset['validation'].num_rows // 10)) # dataset[\"validation\"].shuffle(seed=42).select([i for i in list(range(50))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9lYU6nB0AhX"
      },
      "source": [
        "### ‚öôÔ∏è Tokenizing the dataset\n",
        "In a typical NLP workflow, we must first tokenize our dataset.\n",
        "\n",
        "Converting the stream of characters in the text into a stream of defined \"tokens\", which can be anything from a smaller set of characters to words from a vocabulary.\n",
        "\n",
        "We will use a pretrained model, so we inherit its tokenization scheme.\n",
        "\n",
        "Wanting to see all files on RoBERTa e.g. tokenization https://huggingface.co/roberta-base/tree/main \n",
        "\n",
        "**Merge**-file explanation https://github.com/huggingface/transformers/issues/4777 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Files used for the RoBERTa pre-trained Tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current wokring directory /zhome/94/5/127021/speciale/master_project/notebooks\n"
          ]
        }
      ],
      "source": [
        "ellen_little_nb_path = '/zhome/94/5/127021/speciale/master_project/notebooks'\n",
        "roberta_files_path = '/work3/s174498/roberta_files/'\n",
        "print('Current wokring directory',os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load\n",
        "checkpoint = '/work3/s174498/finetuning-sentiment-model-all-samples-test6/checkpoint-1000'\n",
        "\n",
        "# tokenizer\n",
        "tokenizer_checkpoint = RobertaTokenizer.from_pretrained(checkpoint) \n",
        "tokenizer_pretrained = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# model\n",
        "model = RobertaForSequenceClassification.from_pretrained(checkpoint,output_hidden_states = True, output_attentions = True, return_dict = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# merges files\n",
        "df_merges = pd.read_csv(\"/work3/s174498/roberta_files/merges.txt\", sep=\" \",  on_bad_lines='skip')\n",
        "\n",
        "# dict file\n",
        "df_dict = pd.read_csv(\"/work3/s174498/roberta_files/dict.txt\", sep=\" \", header = None, names = ['id_GPT_2','occurrence'])\n",
        "\n",
        "# tokenizer file\n",
        "file = open('/work3/s174498/roberta_files/tokenizer.json')\n",
        "tokenizer_json = json.load(file)\n",
        "file.close()\n",
        "\n",
        "# vocab file\n",
        "file = open('/work3/s174498/roberta_files/vocab.json')\n",
        "vocab = json.load(file)\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_GPT_2</th>\n",
              "      <th>occurrence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13</td>\n",
              "      <td>850314647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>262</td>\n",
              "      <td>800385005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "      <td>800251374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>284</td>\n",
              "      <td>432911125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>290</td>\n",
              "      <td>394899794</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  id_GPT_2  occurrence\n",
              "0       13   850314647\n",
              "1      262   800385005\n",
              "2       11   800251374\n",
              "3      284   432911125\n",
              "4      290   394899794"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_dict.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Vocab** consists of 50265 'units'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab length: 50265\n",
            "the 10 first: ['<s>', '<pad>', '</s>', '<unk>', '.', 'ƒ†the', ',', 'ƒ†to', 'ƒ†and', 'ƒ†of']\n"
          ]
        }
      ],
      "source": [
        "print('vocab length:', len(vocab.keys()))\n",
        "print('the 10 first:',list(vocab.keys())[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tokenizer** has a lot of information about model, and which setting are chosen and the vocab can be found here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1.0'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer_json['version']\n",
        "#tokenizer['model']\n",
        "#tokenizer['model']['vocab']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How these files are used  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of text: 52\n",
            "A day is just better with Lavazza coffee. You agree?\n"
          ]
        }
      ],
      "source": [
        "text = 'A day is just better with Lavazza coffee. You agree?'\n",
        "print('length of text:',len(text))\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1.** First step in the tokenizer is to tokenize according to the merges-file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of tokens: 14\n",
            "tokens from pre-trained:\n",
            " ['A', 'ƒ†day', 'ƒ†is', 'ƒ†just', 'ƒ†better', 'ƒ†with', 'ƒ†Lav', 'az', 'za', 'ƒ†coffee', '.', 'ƒ†You', 'ƒ†agree', '?']\n",
            "tokens from checkpoint:\n",
            " ['A', 'ƒ†day', 'ƒ†is', 'ƒ†just', 'ƒ†better', 'ƒ†with', 'ƒ†Lav', 'az', 'za', 'ƒ†coffee', '.', 'ƒ†You', 'ƒ†agree', '?']\n"
          ]
        }
      ],
      "source": [
        "print('number of tokens:', len(tokenizer_pretrained.tokenize(text)))\n",
        "print('tokens from pre-trained:\\n',tokenizer_pretrained.tokenize(text))\n",
        "print('tokens from checkpoint:\\n',tokenizer_checkpoint.tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*ƒ† is the rep. for space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2.** Second step is to replace these tokens with their corresponding indices, using the vocab-file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of idx: 16\n",
            "indices from pre-trained:\n",
            " [0, 250, 183, 16, 95, 357, 19, 18126, 1222, 2478, 3895, 4, 370, 2854, 116, 2]\n",
            "indices from checkpoint:\n",
            " [0, 250, 183, 16, 95, 357, 19, 18126, 1222, 2478, 3895, 4, 370, 2854, 116, 2]\n"
          ]
        }
      ],
      "source": [
        "print('number of idx:',len(tokenizer_pretrained.encode(text)))\n",
        "print('indices from pre-trained:\\n',tokenizer_pretrained.encode(text))\n",
        "print('indices from checkpoint:\\n',tokenizer_checkpoint.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Where the following for end and start of sentence is used \n",
        "* sep : ['< /s>', 2] (last token of a sequence built with special tokens)\n",
        "* cls : ['< s>', 0] (fisrt token of a sequence built with special tokens)\n",
        "\n",
        "Having the indices we can decode back to original text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decode from pre-trained:\n",
            " <s>A day is just better with Lavazza coffee. You agree?</s>\n",
            "decode from checkpoint:\n",
            " <s>A day is just better with Lavazza coffee. You agree?</s>\n"
          ]
        }
      ],
      "source": [
        "print('decode from pre-trained:\\n',tokenizer_pretrained.decode(tokenizer_pretrained.encode(text)))\n",
        "print('decode from checkpoint:\\n',tokenizer_checkpoint.decode(tokenizer_checkpoint.encode(text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dict.txt file is the connection between GPT-2 vocab and RoBERTa vocab. \n",
        "\n",
        "* Where the **row-idx+4 is the index in RoBERTa** - the 4 is from the 4 special tokens (see below).\n",
        "* And the column 'index' is the **index from GPT-2.**\n",
        "* The column 'occurencies' gives the number of times the **index/token appears**' in the training set. \n",
        "\n",
        "The GPT-2 vocab is remapped with the RoBERTa vocab and the first four values are the special tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<s>': 0, '<pad>': 1, '</s>': 2, '<unk>': 3}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# special tokens\n",
        "{\"<s>\": 0, \"<pad>\": 1, \"</s>\": 2, \"<unk>\": 3}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Settings for Tokenizer\n",
        "Tokenizer.json gives all settings for the Tokenizer.\n",
        "\n",
        "As examples are that it gives which special tokens are added and their corresponding id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 0, 'special': True, 'content': '<s>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': True}\n",
            "{'id': 1, 'special': True, 'content': '<pad>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': True}\n",
            "{'id': 2, 'special': True, 'content': '</s>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': True}\n",
            "{'id': 3, 'special': True, 'content': '<unk>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': True}\n",
            "{'id': 50264, 'special': True, 'content': '<mask>', 'single_word': False, 'lstrip': True, 'rstrip': False, 'normalized': True}\n"
          ]
        }
      ],
      "source": [
        "nr_add_tokens = len(tokenizer_json['added_tokens'])\n",
        "for i in range(nr_add_tokens):\n",
        "    print(tokenizer_json['added_tokens'][i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Where **cls** is the classifier token which is used when doing sequence classification (classification of the whole sequence instead of per-token classification)\n",
        "\n",
        "and **sep** is the separator token, which is used when building a sequence from multiple sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'type': 'RobertaProcessing',\n",
              " 'sep': ['</s>', 2],\n",
              " 'cls': ['<s>', 0],\n",
              " 'trim_offsets': True,\n",
              " 'add_prefix_space': False}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer_json['post_processor']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Some of the inputs to the Tokenizer\n",
        "\n",
        "* **bos_token** (str, optional, defaults to \"< s>\") ‚Äî The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n",
        "\n",
        "* **eos_token** (str, optional, defaults to \"< /s>\") ‚Äî The end of sequence token.\n",
        "\n",
        "* **sep_token** (str, optional, defaults to \"< /s>\") ‚Äî The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for sequence classification or for a text and a question for question answering. It is also used as the last token of a sequence built with special tokens.\n",
        "\n",
        "* **cls_token** (str, optional, defaults to \"< s>\") ‚Äî The classifier token which is used when doing sequence classification (classification of the whole sequence instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
        "\n",
        "* **unk_token** (str, optional, defaults to \"< unk>\") ‚Äî The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this token instead.\n",
        "\n",
        "* **pad_token** (str, optional, defaults to \"< pad>\") ‚Äî The token used for padding, for example when batching sequences of different lengths.\n",
        "\n",
        "* **mask_token** (str, optional, defaults to \"< mask>\") ‚Äî The token used for masking values. This is the token used when training this model with masked language modeling. This is the token which the model will try to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prediction \n",
        "A little bit of prediction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?ba/s]\n"
          ]
        },
        {
          "ename": "PermissionError",
          "evalue": "[Errno 13] Permission denied: '/work3/s174498/sst2_dataset/test_dataset/tmpc5u0wdx7'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [17], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_function\u001b[39m(examples):\n\u001b[1;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_checkpoint(examples[\u001b[39m\"\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m\"\u001b[39m], truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m tokenized_test \u001b[39m=\u001b[39m test_dataset\u001b[39m.\u001b[39;49mmap(preprocess_function, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/datasets/arrow_dataset.py:2572\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2569\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   2571\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2572\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[1;32m   2573\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m   2574\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m   2575\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m   2576\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   2577\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m   2578\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2579\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m   2580\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m   2581\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   2582\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   2583\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   2584\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   2585\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   2586\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m   2587\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   2588\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   2589\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[1;32m   2590\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   2591\u001b[0m     )\n\u001b[1;32m   2592\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2594\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/datasets/arrow_dataset.py:584\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    583\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    585\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    586\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    587\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/datasets/arrow_dataset.py:551\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    545\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    546\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    547\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    548\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    549\u001b[0m }\n\u001b[1;32m    550\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 551\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    552\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    553\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/datasets/fingerprint.py:480\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    478\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    482\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/datasets/arrow_dataset.py:2980\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   2979\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2980\u001b[0m         buf_writer, writer, tmp_file \u001b[39m=\u001b[39m init_buffer_and_writer()\n\u001b[1;32m   2981\u001b[0m         stack\u001b[39m.\u001b[39menter_context(writer)\n\u001b[1;32m   2982\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(batch, pa\u001b[39m.\u001b[39mTable):\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/datasets/arrow_dataset.py:2904\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.init_buffer_and_writer\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2902\u001b[0m     buf_writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2903\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCaching processed dataset at \u001b[39m\u001b[39m{\u001b[39;00mcache_file_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2904\u001b[0m     tmp_file \u001b[39m=\u001b[39m tempfile\u001b[39m.\u001b[39;49mNamedTemporaryFile(\u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mdir\u001b[39;49m\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mdirname(cache_file_name), delete\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   2905\u001b[0m     writer \u001b[39m=\u001b[39m ArrowWriter(\n\u001b[1;32m   2906\u001b[0m         features\u001b[39m=\u001b[39mwriter_features,\n\u001b[1;32m   2907\u001b[0m         path\u001b[39m=\u001b[39mtmp_file\u001b[39m.\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2911\u001b[0m         disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[1;32m   2912\u001b[0m     )\n\u001b[1;32m   2913\u001b[0m \u001b[39mreturn\u001b[39;00m buf_writer, writer, tmp_file\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/tempfile.py:545\u001b[0m, in \u001b[0;36mNamedTemporaryFile\u001b[0;34m(mode, buffering, encoding, newline, suffix, prefix, dir, delete, errors)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mif\u001b[39;00m _os\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnt\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m delete:\n\u001b[1;32m    543\u001b[0m     flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m _os\u001b[39m.\u001b[39mO_TEMPORARY\n\u001b[0;32m--> 545\u001b[0m (fd, name) \u001b[39m=\u001b[39m _mkstemp_inner(\u001b[39mdir\u001b[39;49m, prefix, suffix, flags, output_type)\n\u001b[1;32m    546\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    547\u001b[0m     file \u001b[39m=\u001b[39m _io\u001b[39m.\u001b[39mopen(fd, mode, buffering\u001b[39m=\u001b[39mbuffering,\n\u001b[1;32m    548\u001b[0m                     newline\u001b[39m=\u001b[39mnewline, encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/tempfile.py:255\u001b[0m, in \u001b[0;36m_mkstemp_inner\u001b[0;34m(dir, pre, suf, flags, output_type)\u001b[0m\n\u001b[1;32m    253\u001b[0m _sys\u001b[39m.\u001b[39maudit(\u001b[39m\"\u001b[39m\u001b[39mtempfile.mkstemp\u001b[39m\u001b[39m\"\u001b[39m, file)\n\u001b[1;32m    254\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 255\u001b[0m     fd \u001b[39m=\u001b[39m _os\u001b[39m.\u001b[39;49mopen(file, flags, \u001b[39m0o600\u001b[39;49m)\n\u001b[1;32m    256\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileExistsError\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     \u001b[39mcontinue\u001b[39;00m    \u001b[39m# try again\u001b[39;00m\n",
            "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/work3/s174498/sst2_dataset/test_dataset/tmpc5u0wdx7'"
          ]
        }
      ],
      "source": [
        "# Prepare the text inputs for the model\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer_checkpoint(examples[\"sentence\"], truncation=True)\n",
        "\n",
        "tokenized_test = test_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                        \n",
        "    tokenizer=tokenizer_checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 1821\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Predicting with model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictions \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mpredict(test_dataset)\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/transformers/trainer.py:2848\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2845\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2847\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2848\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   2849\u001b[0m     test_dataloader, description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPrediction\u001b[39;49m\u001b[39m\"\u001b[39;49m, ignore_keys\u001b[39m=\u001b[39;49mignore_keys, metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix\n\u001b[1;32m   2850\u001b[0m )\n\u001b[1;32m   2851\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   2852\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m   2853\u001b[0m     speed_metrics(\n\u001b[1;32m   2854\u001b[0m         metric_key_prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2858\u001b[0m     )\n\u001b[1;32m   2859\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/transformers/trainer.py:2942\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2940\u001b[0m observed_num_examples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   2941\u001b[0m \u001b[39m# Main evaluation loop\u001b[39;00m\n\u001b[0;32m-> 2942\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m   2943\u001b[0m     \u001b[39m# Update the observed num examples\u001b[39;00m\n\u001b[1;32m   2944\u001b[0m     observed_batch_size \u001b[39m=\u001b[39m find_batch_size(inputs)\n\u001b[1;32m   2945\u001b[0m     \u001b[39mif\u001b[39;00m observed_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/transformers/data/data_collator.py:247\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, features: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m--> 247\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad(\n\u001b[1;32m    248\u001b[0m         features,\n\u001b[1;32m    249\u001b[0m         padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    250\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_length,\n\u001b[1;32m    251\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    252\u001b[0m         return_tensors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_tensors,\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    254\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m batch:\n\u001b[1;32m    255\u001b[0m         batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2900\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   2898\u001b[0m \u001b[39m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[1;32m   2899\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m encoded_inputs:\n\u001b[0;32m-> 2900\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2901\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2902\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthat includes \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, but you provided \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(encoded_inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2903\u001b[0m     )\n\u001b[1;32m   2905\u001b[0m required_input \u001b[39m=\u001b[39m encoded_inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]]\n\u001b[1;32m   2907\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m required_input:\n",
            "\u001b[0;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']"
          ]
        }
      ],
      "source": [
        "# Predicting with model\n",
        "predictions = trainer.predict(test_dataset)\n",
        "#dataset_test_pred = list(np.argmax(predictions.predictions, axis=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_metric = load_metric(\"accuracy\")\n",
        "accuracy_metric.compute(predictions=dataset_test_pred, references=test_dataset['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Some other text examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_text = [\"Hello World, Hello World, and hello world differs.\", \"It tokenize Danish words and rare english words such as 'speciale skrivning' and Obelus, Nudiustertian, Nikehedonia and Metanoia\"]\n",
        "test_text2 = ['in store and dog',' in store ','in store ',' in store','in store?','in store.','in store .']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tokenizer first tokenizes according to the merges file:\n",
            " ['Hello', 'ƒ†World', ',', 'ƒ†Hello', 'ƒ†World', ',', 'ƒ†and', 'ƒ†hello', 'ƒ†world', 'ƒ†differs', '.']\n",
            "And then(according to the values in the vocab.json)\n",
            "these tokens are then replaced by their indices:\n",
            " [0, 31414, 623, 6, 20920, 623, 6, 8, 20760, 232, 31381, 4, 2]\n",
            "<s>Hello World, Hello World, and hello world differs.</s>\n",
            "['<s>', 'Hello', 'ƒ†World', ',', 'ƒ†Hello', 'ƒ†World', ',', 'ƒ†and', 'ƒ†hello', 'ƒ†world', 'ƒ†differs', '.', '</s>']\n",
            "The tokenizer first tokenizes according to the merges file:\n",
            " ['It', 'ƒ†token', 'ize', 'ƒ†Danish', 'ƒ†words', 'ƒ†and', 'ƒ†rare', 'ƒ†english', 'ƒ†words', 'ƒ†such', 'ƒ†as', \"ƒ†'\", 'special', 'e', 'ƒ†sk', 'riv', 'ning', \"'\", 'ƒ†and', 'ƒ†Ob', 'el', 'us', ',', 'ƒ†N', 'udi', 'ust', 'ert', 'ian', ',', 'ƒ†Nike', 'hed', 'onia', 'ƒ†and', 'ƒ†Met', 'anoia']\n",
            "And then(according to the values in the vocab.json)\n",
            "these tokens are then replaced by their indices:\n",
            " [0, 243, 19233, 2072, 13501, 1617, 8, 3159, 47510, 1617, 215, 25, 128, 19423, 242, 2972, 16936, 3509, 108, 8, 5816, 523, 687, 6, 234, 24110, 4193, 2399, 811, 6, 10239, 4183, 15402, 8, 4369, 47733, 2]\n",
            "<s>It tokenize Danish words and rare english words such as'speciale skrivning' and Obelus, Nudiustertian, Nikehedonia and Metanoia</s>\n",
            "['<s>', 'It', 'ƒ†token', 'ize', 'ƒ†Danish', 'ƒ†words', 'ƒ†and', 'ƒ†rare', 'ƒ†english', 'ƒ†words', 'ƒ†such', 'ƒ†as', \"ƒ†'\", 'special', 'e', 'ƒ†sk', 'riv', 'ning', \"'\", 'ƒ†and', 'ƒ†Ob', 'el', 'us', ',', 'ƒ†N', 'udi', 'ust', 'ert', 'ian', ',', 'ƒ†Nike', 'hed', 'onia', 'ƒ†and', 'ƒ†Met', 'anoia', '</s>']\n"
          ]
        }
      ],
      "source": [
        "text = test_text\n",
        "for i in range(len(text)):\n",
        "    ids = tokenizer(text[i], truncation=True)['input_ids']\n",
        "    \n",
        "    print('The tokenizer first tokenizes according to the merges file:\\n',tokenizer.tokenize(text[i]))\n",
        "    print('And then(according to the values in the vocab.json)\\nthese tokens are then replaced by their indices:\\n',ids)\n",
        "\n",
        "    print(tokenizer.decode(ids))\n",
        "    print(tokenizer.convert_ids_to_tokens(ids))\n",
        "\n",
        "#tokenizer(test_text2, truncation = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# input_ids: the token indices\n",
        "# attention_mask: exactly ehat it says - a 0 or 1 array that tells the model which tokens should be attended to and which should not\n",
        "\n",
        "# The truncation argument controls truncation. It can be a boolean or a string:\n",
        "# True or 'longest_first': \n",
        "# truncate to a maximum length specified by the max_length argument or the maximum length accepted by the model if no max_length is provided (max_length=None). \n",
        "# This will truncate token by token, removing a token from the longest sequence in the pair until the proper length is reached."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [[0, 37265, 92, 3556, 2485, 31, 5, 20536, 2833, 1437, 2], [0, 10800, 5069, 117, 22094, 2156, 129, 6348, 3995, 821, 8299, 1437, 2], [0, 6025, 6138, 63, 3768, 8, 39906, 402, 1195, 2721, 59, 1050, 2574, 1437, 2], [0, 5593, 5069, 19223, 10028, 7, 1091, 5, 276, 1328, 1437, 2], [0, 261, 5, 2373, 13543, 12, 1116, 12, 627, 12, 1396, 11622, 43848, 5739, 5, 17504, 115, 31120, 1899, 62, 1437, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(small_data_train['sentence'][:5])#, truncation = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenizer_pretrained(test_dataset['sentence'], padding=True,truncation = True)['input_ids'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPYLcMM00AhZ"
      },
      "source": [
        "We then map the tokenizer over our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n",
            "Downloading and preparing dataset sst2/default (download: 7.09 MiB, generated: 4.78 MiB, post-processed: Unknown size, total: 11.88 MiB) to /zhome/94/5/127021/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5...\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "[Errno 28] No space left on device: '/zhome/94/5/127021/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5.incomplete'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_function\u001b[39m(examples):\n\u001b[1;32m      2\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(examples[\u001b[39m\"\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m\"\u001b[39m], truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39msst2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m small_train_dataset \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshuffle(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\u001b[39m.\u001b[39mselect([i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m))])\n\u001b[1;32m      7\u001b[0m small_val_dataset \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshuffle(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\u001b[39m.\u001b[39mselect([i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m))])\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/datasets/load.py:1742\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1739\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1741\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1742\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   1743\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1744\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1745\u001b[0m     ignore_verifications\u001b[39m=\u001b[39;49mignore_verifications,\n\u001b[1;32m   1746\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   1747\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1748\u001b[0m )\n\u001b[1;32m   1750\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   1752\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   1753\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/datasets/builder.py:793\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_manual_download(dl_manager)\n\u001b[1;32m    792\u001b[0m \u001b[39m# Create a tmp dir and rename to self._output_dir on successful exit.\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m \u001b[39mwith\u001b[39;00m incomplete_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir) \u001b[39mas\u001b[39;00m tmp_output_dir:\n\u001b[1;32m    794\u001b[0m     \u001b[39m# Temporarily assign _output_dir to tmp_data_dir to avoid having to forward\u001b[39;00m\n\u001b[1;32m    795\u001b[0m     \u001b[39m# it to every sub function.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m     \u001b[39mwith\u001b[39;00m temporary_assignment(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_output_dir\u001b[39m\u001b[39m\"\u001b[39m, tmp_output_dir):\n\u001b[1;32m    797\u001b[0m \n\u001b[1;32m    798\u001b[0m         \u001b[39m# Try to download the already prepared dataset files\u001b[39;00m\n\u001b[1;32m    799\u001b[0m         downloaded_from_gcs \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    120\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/site-packages/datasets/builder.py:763\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare.<locals>.incomplete_dir\u001b[0;34m(dirname)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     tmp_dir \u001b[39m=\u001b[39m dirname \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.incomplete\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 763\u001b[0m     os\u001b[39m.\u001b[39;49mmakedirs(tmp_dir, exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    764\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m         \u001b[39myield\u001b[39;00m tmp_dir\n",
            "File \u001b[0;32m~/miniconda3/envs/roberta_env/lib/python3.9/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     mkdir(name, mode)\n\u001b[1;32m    226\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '/zhome/94/5/127021/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5.incomplete'"
          ]
        }
      ],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], truncation=True)\n",
        "\n",
        "dataset = load_dataset(\"sst2\")\n",
        "\n",
        "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select([i for i in list(range(100))])\n",
        "small_val_dataset = dataset[\"validation\"].shuffle(seed=42).select([i for i in list(range(50))])\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_val = small_val_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "#model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('roberta_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2c3ec90920587dcd62ca10f98568309ae5fe8dd1757bd16b3e1a83d20ad0c067"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
